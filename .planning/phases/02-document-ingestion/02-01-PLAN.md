---
phase: 02-document-ingestion
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - mcps/knowledge-mcp/pyproject.toml
  - mcps/knowledge-mcp/src/knowledge_mcp/utils/tokenizer.py
  - mcps/knowledge-mcp/src/knowledge_mcp/utils/hashing.py
  - mcps/knowledge-mcp/src/knowledge_mcp/utils/normative.py
  - mcps/knowledge-mcp/tests/unit/test_utils/test_tokenizer.py
  - mcps/knowledge-mcp/tests/unit/test_utils/test_hashing.py
  - mcps/knowledge-mcp/tests/unit/test_utils/test_normative.py
autonomous: true

must_haves:
  truths:
    - "tiktoken counts tokens identically to OpenAI embedding API"
    - "Content hashes are deterministic and normalized"
    - "Normative detection correctly identifies SHALL/SHOULD/MAY keywords"
  artifacts:
    - path: "mcps/knowledge-mcp/src/knowledge_mcp/utils/tokenizer.py"
      provides: "Token counting using tiktoken cl100k_base"
      exports: ["count_tokens", "TokenizerConfig"]
    - path: "mcps/knowledge-mcp/src/knowledge_mcp/utils/hashing.py"
      provides: "SHA-256 content hashing with normalization"
      exports: ["compute_content_hash"]
    - path: "mcps/knowledge-mcp/src/knowledge_mcp/utils/normative.py"
      provides: "Normative/informative chunk detection"
      exports: ["detect_normative", "NormativeIndicator"]
  key_links:
    - from: "tokenizer.py"
      to: "tiktoken"
      via: "encoding_for_model('text-embedding-3-small')"
      pattern: "tiktoken\\.encoding_for_model"
    - from: "hashing.py"
      to: "hashlib"
      via: "sha256"
      pattern: "hashlib\\.sha256"
---

<objective>
Add Docling dependency and create foundation utilities for document ingestion: tokenizer (tiktoken), content hashing (SHA-256), and normative detection.

Purpose: These utilities are used by all chunking and ingestion components. Token counting ensures chunks fit embedding API limits. Hashing enables deduplication. Normative detection tags chunks for filtering.

Output: Three utility modules with comprehensive unit tests, Docling 2.70.0+ added to dependencies.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-document-ingestion/02-CONTEXT.md
@.planning/phases/02-document-ingestion/02-RESEARCH.md
@mcps/knowledge-mcp/src/knowledge_mcp/utils/__init__.py
@mcps/knowledge-mcp/src/knowledge_mcp/models/chunk.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add Docling dependency and update pyproject.toml</name>
  <files>mcps/knowledge-mcp/pyproject.toml, mcps/knowledge-mcp/poetry.lock</files>
  <action>
Add Docling to dependencies:
```bash
cd mcps/knowledge-mcp && poetry add "docling>=2.70.0"
```

Verify Python version compatibility (project requires >=3.11,<3.14, Docling requires >=3.10).

Do NOT remove pymupdf4llm - keep it as fallback for simple documents per RESEARCH.md recommendation.
  </action>
  <verify>
`poetry show docling` shows version >=2.70.0 installed.
`poetry run python -c "from docling.document_converter import DocumentConverter; print('OK')"` succeeds.
  </verify>
  <done>Docling 2.70.0+ installed alongside existing dependencies, poetry.lock updated.</done>
</task>

<task type="auto">
  <name>Task 2: Create tokenizer utility with tiktoken</name>
  <files>mcps/knowledge-mcp/src/knowledge_mcp/utils/tokenizer.py, mcps/knowledge-mcp/tests/unit/test_utils/test_tokenizer.py</files>
  <action>
Create `tokenizer.py` with:
- `TokenizerConfig` dataclass with model name (default "text-embedding-3-small") and max_tokens (default 500)
- `count_tokens(text: str, model: str = "text-embedding-3-small") -> int` function
- `truncate_to_tokens(text: str, max_tokens: int, model: str = "text-embedding-3-small") -> str` function
- Use `tiktoken.encoding_for_model()` to get cl100k_base encoding
- Cache encoding instance for performance (module-level or lru_cache)

Create unit tests covering:
- Token count matches expected for known strings
- Handles empty string (returns 0)
- Handles unicode characters correctly
- truncate_to_tokens preserves complete text when under limit
- truncate_to_tokens correctly truncates when over limit

Follow existing code conventions from utils/config.py (Google-style docstrings, type hints, __all__ export).
  </action>
  <verify>
`poetry run pytest mcps/knowledge-mcp/tests/unit/test_utils/test_tokenizer.py -v` passes all tests.
`poetry run pyright mcps/knowledge-mcp/src/knowledge_mcp/utils/tokenizer.py` reports zero errors.
  </verify>
  <done>Tokenizer utility counts tokens using tiktoken cl100k_base, with truncation support and unit tests.</done>
</task>

<task type="auto">
  <name>Task 3: Create hashing and normative detection utilities</name>
  <files>
    mcps/knowledge-mcp/src/knowledge_mcp/utils/hashing.py
    mcps/knowledge-mcp/src/knowledge_mcp/utils/normative.py
    mcps/knowledge-mcp/tests/unit/test_utils/test_hashing.py
    mcps/knowledge-mcp/tests/unit/test_utils/test_normative.py
    mcps/knowledge-mcp/src/knowledge_mcp/utils/__init__.py
  </files>
  <action>
Create `hashing.py` with:
- `compute_content_hash(text: str) -> str` function
- Normalize text before hashing: strip whitespace, convert \r\n to \n
- Use hashlib.sha256 and return hexdigest
- Match existing pattern from embed/cache.py if similar exists

Create `normative.py` with:
- `NormativeIndicator` enum: NORMATIVE, INFORMATIVE, UNKNOWN
- `detect_normative(text: str, section_path: str = "") -> NormativeIndicator` function
- Detection heuristics per RESEARCH.md:
  - SHALL/MUST/REQUIRED -> NORMATIVE
  - SHOULD -> NORMATIVE
  - MAY/CAN -> INFORMATIVE
  - NOTE/EXAMPLE -> INFORMATIVE
  - Annex/Appendix "(normative)" or "(informative)" markers
  - Default body content -> NORMATIVE

Create unit tests for both modules:
- hashing: deterministic output, whitespace normalization, unicode handling
- normative: SHALL detection, SHOULD detection, MAY detection, NOTE detection, section markers

Update utils/__init__.py to export new modules.
  </action>
  <verify>
`poetry run pytest mcps/knowledge-mcp/tests/unit/test_utils/test_hashing.py mcps/knowledge-mcp/tests/unit/test_utils/test_normative.py -v` passes.
`poetry run pyright mcps/knowledge-mcp/src/knowledge_mcp/utils/` reports zero errors.
  </verify>
  <done>Hashing utility computes SHA-256 with normalization. Normative detection identifies SHALL/SHOULD/MAY and section markers.</done>
</task>

</tasks>

<verification>
Overall plan verification:
```bash
cd mcps/knowledge-mcp
poetry run pytest tests/unit/test_utils/ -v
poetry run pyright src/knowledge_mcp/utils/
poetry run python -c "from knowledge_mcp.utils.tokenizer import count_tokens; from knowledge_mcp.utils.hashing import compute_content_hash; from knowledge_mcp.utils.normative import detect_normative; print('All imports OK')"
```
</verification>

<success_criteria>
1. Docling >=2.70.0 is installed and importable
2. count_tokens() returns accurate token counts matching OpenAI API
3. compute_content_hash() produces deterministic SHA-256 hashes
4. detect_normative() correctly classifies normative vs informative text
5. All new code passes pyright strict mode with zero errors
6. Unit test coverage for new utilities
</success_criteria>

<output>
After completion, create `.planning/phases/02-document-ingestion/02-01-SUMMARY.md`
</output>
