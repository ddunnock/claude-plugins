---
phase: 02-document-ingestion
plan: 04
type: execute
wave: 3
depends_on: ["02-02", "02-03"]
files_modified:
  - mcps/knowledge-mcp/src/knowledge_mcp/ingest/docx_ingestor.py
  - mcps/knowledge-mcp/src/knowledge_mcp/ingest/pipeline.py
  - mcps/knowledge-mcp/src/knowledge_mcp/ingest/__init__.py
  - mcps/knowledge-mcp/tests/unit/test_ingest/test_docx_ingestor.py
  - mcps/knowledge-mcp/tests/unit/test_ingest/test_pipeline.py
autonomous: true

must_haves:
  truths:
    - "DOCX files are parsed with structure preserved"
    - "Pipeline converts raw documents to KnowledgeChunks ready for embedding"
    - "Chunks have all required metadata: source_path, section, page, clause, normative tag"
  artifacts:
    - path: "mcps/knowledge-mcp/src/knowledge_mcp/ingest/docx_ingestor.py"
      provides: "Docling-based DOCX parser"
      exports: ["DOCXIngestor"]
    - path: "mcps/knowledge-mcp/src/knowledge_mcp/ingest/pipeline.py"
      provides: "Ingestion pipeline orchestrator"
      exports: ["IngestionPipeline", "ingest_document"]
  key_links:
    - from: "pipeline.py"
      to: "pdf_ingestor.py"
      via: "PDFIngestor for .pdf files"
      pattern: "PDFIngestor"
    - from: "pipeline.py"
      to: "hierarchical.py"
      via: "HierarchicalChunker for chunking"
      pattern: "HierarchicalChunker"
    - from: "pipeline.py"
      to: "normative.py"
      via: "detect_normative for tagging"
      pattern: "detect_normative"
    - from: "pipeline.py"
      to: "KnowledgeChunk"
      via: "creates final chunks"
      pattern: "KnowledgeChunk"
---

<objective>
Add DOCX support and create the ingestion pipeline that orchestrates parsing, chunking, and enrichment to produce KnowledgeChunks ready for embedding.

Purpose: The pipeline is the main entry point for document ingestion. It automatically selects the right ingestor based on file type, chunks the content, and enriches each chunk with all metadata required for search and citations.

Output: DOCX ingestor, ingestion pipeline, comprehensive unit tests.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/02-document-ingestion/02-CONTEXT.md
@.planning/phases/02-document-ingestion/02-RESEARCH.md
@.planning/phases/02-document-ingestion/02-02-SUMMARY.md
@.planning/phases/02-document-ingestion/02-03-SUMMARY.md
@mcps/knowledge-mcp/src/knowledge_mcp/ingest/base.py
@mcps/knowledge-mcp/src/knowledge_mcp/ingest/pdf_ingestor.py
@mcps/knowledge-mcp/src/knowledge_mcp/chunk/hierarchical.py
@mcps/knowledge-mcp/src/knowledge_mcp/utils/normative.py
@mcps/knowledge-mcp/src/knowledge_mcp/utils/hashing.py
@mcps/knowledge-mcp/src/knowledge_mcp/models/chunk.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement DOCX ingestor</name>
  <files>
    mcps/knowledge-mcp/src/knowledge_mcp/ingest/docx_ingestor.py
    mcps/knowledge-mcp/tests/unit/test_ingest/test_docx_ingestor.py
  </files>
  <action>
Create `ingest/docx_ingestor.py` with `DOCXIngestor(BaseIngestor)`:

1. `__init__(self)` - Initialize Docling DocumentConverter
   - Docling supports DOCX natively, use same converter as PDF

2. `ingest(file_path: Path) -> ParsedDocument`:
   - Similar implementation to PDFIngestor
   - Use Docling for conversion (handles DOCX structure)
   - Extract metadata from document properties if available
   - Build ParsedElement list from Docling output

3. `supported_extensions() -> list[str]` returns [".docx"]

Create unit tests in `test_docx_ingestor.py`:
- Test supported_extensions returns [".docx"]
- Test ingest returns ParsedDocument with correct structure
- Test error handling for invalid files
- Mock Docling to avoid actual file processing
  </action>
  <verify>
`poetry run pytest mcps/knowledge-mcp/tests/unit/test_ingest/test_docx_ingestor.py -v` passes.
`poetry run pyright mcps/knowledge-mcp/src/knowledge_mcp/ingest/docx_ingestor.py` reports zero errors.
  </verify>
  <done>DOCXIngestor parses DOCX files using Docling with same structure as PDF ingestor.</done>
</task>

<task type="auto">
  <name>Task 2: Create ingestion pipeline orchestrator</name>
  <files>
    mcps/knowledge-mcp/src/knowledge_mcp/ingest/pipeline.py
    mcps/knowledge-mcp/src/knowledge_mcp/ingest/__init__.py
  </files>
  <action>
Create `ingest/pipeline.py` with:

1. `IngestionPipeline` class:
   - `__init__(self, chunk_config: ChunkConfig | None = None)`:
     - Store chunk_config
     - Create ingestor registry: {".pdf": PDFIngestor, ".docx": DOCXIngestor}
     - Create HierarchicalChunker with config

   - `ingest(file_path: Path | str, document_metadata: dict | None = None) -> list[KnowledgeChunk]`:
     - Convert str to Path if needed
     - Select ingestor based on file extension
     - Parse document: `parsed = ingestor.ingest(file_path)`
     - Chunk document: `chunk_results = chunker.chunk(parsed.elements, parsed.metadata)`
     - Enrich chunks: `chunks = self._enrich_chunks(chunk_results, parsed.metadata)`
     - Return list of KnowledgeChunk

   - `_enrich_chunks(chunk_results: list[ChunkResult], metadata: DocumentMetadata) -> list[KnowledgeChunk]`:
     - For each ChunkResult:
       - Generate UUID for id
       - Compute content_hash using hashing utility
       - Detect normative status using normative utility
       - Map ChunkResult fields to KnowledgeChunk fields:
         - document_id, document_title, document_type from metadata
         - content from chunk_result.content
         - token_count from chunk_result.token_count
         - section_hierarchy, clause_number, page_numbers from chunk_result
         - normative from detect_normative()
       - Return KnowledgeChunk

2. `ingest_document(file_path: Path | str, **kwargs) -> list[KnowledgeChunk]`:
   - Convenience function that creates pipeline and calls ingest
   - Pass kwargs to IngestionPipeline constructor

Update `ingest/__init__.py` to export IngestionPipeline, ingest_document, DOCXIngestor.
  </action>
  <verify>
`poetry run pyright mcps/knowledge-mcp/src/knowledge_mcp/ingest/pipeline.py` reports zero errors.
`poetry run python -c "from knowledge_mcp.ingest import IngestionPipeline, ingest_document; print('OK')"` succeeds.
  </verify>
  <done>IngestionPipeline orchestrates parse->chunk->enrich flow, producing KnowledgeChunks.</done>
</task>

<task type="auto">
  <name>Task 3: Create unit tests for ingestion pipeline</name>
  <files>
    mcps/knowledge-mcp/tests/unit/test_ingest/test_pipeline.py
  </files>
  <action>
Create `test_pipeline.py` with comprehensive tests:

1. Test ingestor selection:
   - .pdf files use PDFIngestor
   - .docx files use DOCXIngestor
   - Unknown extension raises IngestionError

2. Test full pipeline flow:
   - Mock ingestor to return sample ParsedDocument
   - Mock chunker to return sample ChunkResults
   - Verify output is list of KnowledgeChunk
   - Verify all required fields are populated

3. Test chunk enrichment:
   - content_hash is computed
   - normative is detected (test with SHALL text, test with NOTE text)
   - UUID is generated for each chunk
   - document_id, document_title from metadata

4. Test error handling:
   - File not found raises IngestionError
   - Parse error propagates with context

5. Test convenience function:
   - ingest_document() creates pipeline and returns chunks

Use fixtures for mock ingestors and chunkers.
  </action>
  <verify>
`poetry run pytest mcps/knowledge-mcp/tests/unit/test_ingest/test_pipeline.py -v` passes all tests.
  </verify>
  <done>Ingestion pipeline has comprehensive tests covering orchestration, enrichment, and error handling.</done>
</task>

</tasks>

<verification>
Overall plan verification:
```bash
cd mcps/knowledge-mcp
poetry run pytest tests/unit/test_ingest/ -v
poetry run pyright src/knowledge_mcp/ingest/
poetry run python -c "
from knowledge_mcp.ingest import IngestionPipeline, ingest_document, PDFIngestor, DOCXIngestor
from knowledge_mcp.models.chunk import KnowledgeChunk
print('All imports OK')
"
```
</verification>

<success_criteria>
1. DOCXIngestor parses DOCX files using Docling
2. IngestionPipeline automatically selects ingestor based on file extension
3. Pipeline produces KnowledgeChunks with all required fields:
   - id (UUID), document_id, document_title, document_type
   - content, content_hash, token_count
   - section_hierarchy, clause_number, page_numbers
   - normative (from detection utility)
4. source_path stored in metadata per NFR-4.4
5. All code passes pyright strict mode
6. Comprehensive unit test coverage
</success_criteria>

<output>
After completion, create `.planning/phases/02-document-ingestion/02-04-SUMMARY.md`
</output>
