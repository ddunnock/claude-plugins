---
phase: 05-production-integration
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/knowledge_mcp/utils/config.py
  - src/knowledge_mcp/embed/openai_embedder.py
  - tests/unit/test_config.py
autonomous: true

must_haves:
  truths:
    - "OpenAIEmbedder accepts optional cache and token_tracker parameters"
    - "KnowledgeConfig includes cache_dir, cache_enabled, token_log_file, token_tracking_enabled fields"
    - "embed() checks cache before API call and stores result after"
    - "embed() tracks tokens via token_tracker when configured"
    - "Existing code without cache/tracker continues to work"
  artifacts:
    - path: "src/knowledge_mcp/utils/config.py"
      provides: "Cache and token tracking configuration fields"
      contains: "cache_dir"
    - path: "src/knowledge_mcp/embed/openai_embedder.py"
      provides: "Cache-aware embedding with token tracking"
      contains: "cache: EmbeddingCache | None"
  key_links:
    - from: "src/knowledge_mcp/embed/openai_embedder.py"
      to: "src/knowledge_mcp/embed/cache.py"
      via: "constructor injection"
      pattern: "self._cache"
    - from: "src/knowledge_mcp/embed/openai_embedder.py"
      to: "src/knowledge_mcp/monitoring/token_tracker.py"
      via: "constructor injection"
      pattern: "self._token_tracker"
---

<objective>
Integrate EmbeddingCache and TokenTracker into OpenAIEmbedder using constructor injection pattern.

Purpose: Enable cost savings through caching and visibility through token tracking in the embedding pipeline.

Output:
- KnowledgeConfig with cache and token tracking configuration
- OpenAIEmbedder with optional cache/tracker integration
- Updated config tests
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-production-integration/05-RESEARCH.md

# Source files to modify
@src/knowledge_mcp/utils/config.py
@src/knowledge_mcp/embed/openai_embedder.py
@tests/unit/test_config.py

# Dependencies to understand
@src/knowledge_mcp/embed/cache.py
@src/knowledge_mcp/monitoring/token_tracker.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Extend KnowledgeConfig with cache and token tracking fields</name>
  <files>src/knowledge_mcp/utils/config.py, tests/unit/test_config.py</files>
  <action>
Add cache and token tracking configuration fields to KnowledgeConfig class:

1. Import Path if not already imported (already there)

2. Add new fields after chromadb_collection (keeping existing field order):

```python
# Embedding Cache Configuration
cache_dir: Path = Field(
    default=Path("./data/embeddings/cache"),
    description="Directory for embedding cache storage",
)
cache_enabled: bool = Field(
    default=True,
    description="Enable embedding cache",
)
cache_size_limit: int = Field(
    default=10 * 1024 * 1024 * 1024,  # 10GB
    ge=100 * 1024 * 1024,  # Min 100MB
    description="Cache size limit in bytes",
)

# Token Tracking Configuration
token_log_file: Path = Field(
    default=Path("./data/token_usage.json"),
    description="Token usage log file path",
)
token_tracking_enabled: bool = Field(
    default=True,
    description="Enable token usage tracking",
)
daily_token_warning_threshold: int = Field(
    default=1_000_000,
    ge=0,
    description="Daily token warning threshold",
)
```

3. Update load_config() to load new fields from environment:

```python
# Cache configuration
cache_dir=Path(os.getenv("CACHE_DIR", "./data/embeddings/cache")),
cache_enabled=os.getenv("CACHE_ENABLED", "true").lower() == "true",
cache_size_limit=int(os.getenv("CACHE_SIZE_LIMIT", str(10 * 1024**3))),

# Token tracking configuration
token_log_file=Path(os.getenv("TOKEN_LOG_FILE", "./data/token_usage.json")),
token_tracking_enabled=os.getenv("TOKEN_TRACKING_ENABLED", "true").lower() == "true",
daily_token_warning_threshold=int(os.getenv("DAILY_TOKEN_WARNING_THRESHOLD", "1000000")),
```

4. Add tests in tests/unit/test_config.py:
- Test default values for new fields
- Test loading from environment variables
- Test cache_size_limit minimum validation (ge=100MB)
  </action>
  <verify>
```bash
cd /Users/dunnock/projects/claude-plugins/mcps/knowledge-mcp && \
poetry run pytest tests/unit/test_config.py -v && \
poetry run pyright src/knowledge_mcp/utils/config.py
```
  </verify>
  <done>
- KnowledgeConfig has cache_dir, cache_enabled, cache_size_limit fields
- KnowledgeConfig has token_log_file, token_tracking_enabled, daily_token_warning_threshold fields
- load_config() populates all new fields from environment
- All config tests pass
- Pyright reports zero errors
  </done>
</task>

<task type="auto">
  <name>Task 2: Integrate cache and token tracking into OpenAIEmbedder</name>
  <files>src/knowledge_mcp/embed/openai_embedder.py</files>
  <action>
Modify OpenAIEmbedder to accept optional cache and token_tracker via constructor injection:

1. Add imports at top of file:

```python
from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from collections.abc import Sequence
    from knowledge_mcp.embed.cache import EmbeddingCache
    from knowledge_mcp.monitoring.token_tracker import TokenTracker
```

Update TYPE_CHECKING block to include new imports (merge with existing).

2. Update __slots__ to include new attributes:

```python
__slots__ = ("_client", "_model", "_dimensions", "_cache", "_token_tracker")
```

3. Update __init__ signature to accept optional cache and tracker:

```python
def __init__(
    self,
    api_key: str,
    *,
    model: str = DEFAULT_MODEL,
    dimensions: int = DEFAULT_DIMENSIONS,
    cache: EmbeddingCache | None = None,
    token_tracker: TokenTracker | None = None,
) -> None:
```

4. Store cache and tracker in __init__ body:

```python
self._cache = cache
self._token_tracker = token_tracker
```

5. Modify embed() method to check cache before API call:

```python
async def embed(self, text: str) -> list[float]:
    if not text or not text.strip():
        raise ValidationError("Text cannot be empty")

    # Check cache first (if configured)
    if self._cache is not None:
        cached = self._cache.get(text)
        if cached is not None:
            # Track cache hit (if tracker configured)
            if self._token_tracker is not None:
                self._token_tracker.track_embedding(text, cache_hit=True)
            return cached

    # Cache miss - call OpenAI API
    try:
        result = await self._call_embedding_api([text])
        embedding = result[0]

        # Validate dimensions
        if len(embedding) != self._dimensions:
            raise ValidationError(
                f"Embedding dimension mismatch: expected {self._dimensions}, "
                f"got {len(embedding)}"
            )

        # Store in cache (if configured)
        if self._cache is not None:
            self._cache.set(text, embedding)

        # Track API usage (if tracker configured)
        if self._token_tracker is not None:
            self._token_tracker.track_embedding(text, cache_hit=False)

        return embedding

    except APITimeoutError as e:
        # ... existing error handling ...
```

6. Modify embed_batch() to use per-text caching:

For each text in the batch:
- Check cache individually
- Track cache hits/misses
- Only call API for cache misses
- Store new embeddings in cache

Implementation approach:
```python
# Separate cached vs uncached texts
cached_embeddings: dict[int, list[float]] = {}
texts_to_embed: list[tuple[int, str]] = []

for i, text in enumerate(texts_list):
    if self._cache is not None:
        cached = self._cache.get(text)
        if cached is not None:
            cached_embeddings[i] = cached
            if self._token_tracker is not None:
                self._token_tracker.track_embedding(text, cache_hit=True)
            continue
    texts_to_embed.append((i, text))

# Process uncached texts in batches
# ... existing batch logic for texts_to_embed ...

# Store new embeddings in cache and track
for i, embedding in newly_embedded:
    if self._cache is not None:
        self._cache.set(texts_list[i], embedding)
    if self._token_tracker is not None:
        self._token_tracker.track_embedding(texts_list[i], cache_hit=False)

# Reassemble results in original order
```

7. Update docstrings to document new parameters.

IMPORTANT: Preserve all existing error handling. Do not modify the retry decorator or exception handlers.
  </action>
  <verify>
```bash
cd /Users/dunnock/projects/claude-plugins/mcps/knowledge-mcp && \
poetry run pytest tests/unit/test_embed/test_openai_embedder.py -v && \
poetry run pyright src/knowledge_mcp/embed/openai_embedder.py
```
  </verify>
  <done>
- OpenAIEmbedder.__init__ accepts optional cache and token_tracker parameters
- embed() checks cache before API call
- embed() stores result in cache after successful API call
- embed() tracks tokens via token_tracker
- embed_batch() uses per-text caching
- All existing tests pass (backwards compatible)
- Pyright reports zero errors
  </done>
</task>

</tasks>

<verification>
```bash
cd /Users/dunnock/projects/claude-plugins/mcps/knowledge-mcp && \
poetry run pytest tests/unit/ -v && \
poetry run pyright src/knowledge_mcp/utils/config.py src/knowledge_mcp/embed/openai_embedder.py
```

Expected: All tests pass, zero pyright errors.
</verification>

<success_criteria>
1. KnowledgeConfig has all 6 new fields with correct defaults
2. load_config() loads all new fields from environment variables
3. OpenAIEmbedder accepts optional cache and token_tracker in constructor
4. embed() checks cache before API, stores after, tracks tokens
5. embed_batch() uses per-text caching for optimal hit rate
6. All existing tests pass (backwards compatibility maintained)
7. Zero pyright errors in modified files
</success_criteria>

<output>
After completion, create `.planning/phases/05-production-integration/05-01-SUMMARY.md`
</output>
