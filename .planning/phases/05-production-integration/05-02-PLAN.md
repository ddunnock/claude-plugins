---
phase: 05-production-integration
plan: 02
type: execute
wave: 2
depends_on: ["05-01"]
files_modified:
  - src/knowledge_mcp/server.py
  - tests/unit/test_embed/test_openai_embedder.py
  - tests/integration/test_embedder_integration.py
autonomous: true

must_haves:
  truths:
    - "Server creates EmbeddingCache and TokenTracker from config when enabled"
    - "Server passes cache and tracker to OpenAIEmbedder"
    - "Cache disabled via config skips cache creation"
    - "Second embed() call with same text does not call API (cache hit)"
    - "TokenTracker records cache_hits and embedding_tokens correctly"
  artifacts:
    - path: "src/knowledge_mcp/server.py"
      provides: "Dependency wiring for cache and tracker"
      contains: "EmbeddingCache"
    - path: "tests/unit/test_embed/test_openai_embedder.py"
      provides: "Unit tests for cache/tracker integration"
      contains: "test_embed_uses_cache"
    - path: "tests/integration/test_embedder_integration.py"
      provides: "Integration tests with real cache/tracker"
      contains: "test_cache_hit_prevents_api_call"
  key_links:
    - from: "src/knowledge_mcp/server.py"
      to: "src/knowledge_mcp/embed/cache.py"
      via: "EmbeddingCache instantiation"
      pattern: "EmbeddingCache\\("
    - from: "src/knowledge_mcp/server.py"
      to: "src/knowledge_mcp/monitoring/token_tracker.py"
      via: "TokenTracker instantiation"
      pattern: "TokenTracker\\("
    - from: "src/knowledge_mcp/server.py"
      to: "src/knowledge_mcp/embed/openai_embedder.py"
      via: "constructor injection"
      pattern: "cache=.*token_tracker="
---

<objective>
Wire cache and token tracking into server and verify with comprehensive tests.

Purpose: Complete the integration by connecting configuration to runtime and proving cache hits reduce API calls.

Output:
- Server that creates and injects cache/tracker from config
- Unit tests verifying embedder cache/tracker logic
- Integration tests verifying end-to-end cache behavior
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-production-integration/05-RESEARCH.md
@.planning/phases/05-production-integration/05-01-SUMMARY.md

# Source files
@src/knowledge_mcp/server.py
@src/knowledge_mcp/embed/openai_embedder.py
@src/knowledge_mcp/embed/cache.py
@src/knowledge_mcp/monitoring/token_tracker.py
@src/knowledge_mcp/utils/config.py

# Test files
@tests/unit/test_embed/test_openai_embedder.py
@tests/conftest.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Wire cache and token tracking in server</name>
  <files>src/knowledge_mcp/server.py</files>
  <action>
Update server.py to create and inject EmbeddingCache and TokenTracker:

1. Add imports at top of file:

```python
from knowledge_mcp.embed.cache import EmbeddingCache
from knowledge_mcp.monitoring.token_tracker import TokenTracker
```

2. Modify _ensure_dependencies() to create cache and tracker from config:

```python
def _ensure_dependencies(self) -> None:
    """Initialize dependencies lazily when server runs (not in tests)."""
    if self._searcher is not None:
        return

    # Load config if not using injected dependencies
    if self._config is None:
        self._config = load_config()

    # Create cache if enabled
    cache: EmbeddingCache | None = None
    if self._config.cache_enabled:
        cache = EmbeddingCache(
            self._config.cache_dir,
            self._config.embedding_model,
            size_limit=self._config.cache_size_limit,
        )

    # Create tracker if enabled
    tracker: TokenTracker | None = None
    if self._config.token_tracking_enabled:
        tracker = TokenTracker(
            self._config.token_log_file,
            self._config.embedding_model,
            daily_warning_threshold=self._config.daily_token_warning_threshold,
        )

    # Create embedder with cache and tracker
    if self._embedder is None:
        self._embedder = OpenAIEmbedder(
            api_key=self._config.openai_api_key,
            model=self._config.embedding_model,
            dimensions=self._config.embedding_dimensions,
            cache=cache,
            token_tracker=tracker,
        )

    # Create store if not provided
    if self._store is None:
        self._store = create_store(self._config)

    # Create searcher
    self._searcher = SemanticSearcher(self._embedder, self._store)
```

IMPORTANT: When embedder is injected via constructor (for testing), do NOT create cache/tracker - use the injected embedder as-is. Only create cache/tracker when creating embedder from config.
  </action>
  <verify>
```bash
cd /Users/dunnock/projects/claude-plugins/mcps/knowledge-mcp && \
poetry run pyright src/knowledge_mcp/server.py && \
poetry run pytest tests/unit/test_server.py -v 2>/dev/null || echo "No server unit tests yet"
```
  </verify>
  <done>
- server.py imports EmbeddingCache and TokenTracker
- _ensure_dependencies() creates cache when cache_enabled=True
- _ensure_dependencies() creates tracker when token_tracking_enabled=True
- OpenAIEmbedder receives cache and tracker via constructor
- Pyright reports zero errors
  </done>
</task>

<task type="auto">
  <name>Task 2: Add unit tests for embedder cache/tracker integration</name>
  <files>tests/unit/test_embed/test_openai_embedder.py</files>
  <action>
Add unit tests for cache and token tracking integration using mocks:

1. Add imports:
```python
from unittest.mock import MagicMock, AsyncMock
```

2. Add test class for cache integration:

```python
class TestOpenAIEmbedderCacheIntegration:
    """Tests for cache integration in OpenAIEmbedder."""

    @pytest.fixture
    def mock_cache(self) -> MagicMock:
        """Create mock EmbeddingCache."""
        cache = MagicMock()
        cache.get.return_value = None  # Default: cache miss
        return cache

    @pytest.fixture
    def mock_tracker(self) -> MagicMock:
        """Create mock TokenTracker."""
        tracker = MagicMock()
        tracker.track_embedding.return_value = 10  # Token count
        return tracker

    @pytest.fixture
    def embedder_with_cache(
        self, mock_cache: MagicMock, mock_tracker: MagicMock
    ) -> OpenAIEmbedder:
        """Create embedder with mock cache and tracker."""
        embedder = OpenAIEmbedder(
            api_key="sk-test-key",
            cache=mock_cache,
            token_tracker=mock_tracker,
        )
        # Mock the API call
        embedder._client = MagicMock()
        embedder._client.embeddings = MagicMock()
        embedder._client.embeddings.create = AsyncMock(
            return_value=MagicMock(
                data=[MagicMock(embedding=[0.1] * 1536)]
            )
        )
        return embedder

    @pytest.mark.asyncio
    async def test_embed_checks_cache_first(
        self,
        embedder_with_cache: OpenAIEmbedder,
        mock_cache: MagicMock,
    ) -> None:
        """Verify cache is checked before API call."""
        await embedder_with_cache.embed("test text")
        mock_cache.get.assert_called_once_with("test text")

    @pytest.mark.asyncio
    async def test_embed_returns_cached_value(
        self,
        embedder_with_cache: OpenAIEmbedder,
        mock_cache: MagicMock,
    ) -> None:
        """Verify cached value is returned without API call."""
        cached_embedding = [0.5] * 1536
        mock_cache.get.return_value = cached_embedding

        result = await embedder_with_cache.embed("cached text")

        assert result == cached_embedding
        # API should NOT be called
        embedder_with_cache._client.embeddings.create.assert_not_called()

    @pytest.mark.asyncio
    async def test_embed_stores_in_cache_on_miss(
        self,
        embedder_with_cache: OpenAIEmbedder,
        mock_cache: MagicMock,
    ) -> None:
        """Verify API result is stored in cache."""
        await embedder_with_cache.embed("new text")

        # Cache.set should be called with text and embedding
        mock_cache.set.assert_called_once()
        call_args = mock_cache.set.call_args
        assert call_args[0][0] == "new text"
        assert len(call_args[0][1]) == 1536

    @pytest.mark.asyncio
    async def test_embed_tracks_cache_hit(
        self,
        embedder_with_cache: OpenAIEmbedder,
        mock_cache: MagicMock,
        mock_tracker: MagicMock,
    ) -> None:
        """Verify token tracker records cache hit."""
        mock_cache.get.return_value = [0.5] * 1536

        await embedder_with_cache.embed("cached text")

        mock_tracker.track_embedding.assert_called_once_with(
            "cached text", cache_hit=True
        )

    @pytest.mark.asyncio
    async def test_embed_tracks_cache_miss(
        self,
        embedder_with_cache: OpenAIEmbedder,
        mock_tracker: MagicMock,
    ) -> None:
        """Verify token tracker records cache miss with API call."""
        await embedder_with_cache.embed("new text")

        mock_tracker.track_embedding.assert_called_once_with(
            "new text", cache_hit=False
        )

    @pytest.mark.asyncio
    async def test_embed_works_without_cache(self) -> None:
        """Verify embedder works when cache is None (backwards compat)."""
        embedder = OpenAIEmbedder(api_key="sk-test-key")
        embedder._client = MagicMock()
        embedder._client.embeddings = MagicMock()
        embedder._client.embeddings.create = AsyncMock(
            return_value=MagicMock(
                data=[MagicMock(embedding=[0.1] * 1536)]
            )
        )

        result = await embedder.embed("test text")

        assert len(result) == 1536
```

3. Add tests for embed_batch cache behavior (similar pattern):
- test_embed_batch_uses_per_text_caching
- test_embed_batch_skips_api_for_cached_texts
- test_embed_batch_stores_new_embeddings
  </action>
  <verify>
```bash
cd /Users/dunnock/projects/claude-plugins/mcps/knowledge-mcp && \
poetry run pytest tests/unit/test_embed/test_openai_embedder.py -v -k "cache" && \
poetry run pytest tests/unit/test_embed/test_openai_embedder.py -v
```
  </verify>
  <done>
- TestOpenAIEmbedderCacheIntegration test class exists
- Tests verify cache is checked before API call
- Tests verify cached value returned without API call
- Tests verify API result stored in cache
- Tests verify token tracking for cache hits and misses
- Tests verify backwards compatibility (cache=None works)
- All tests pass
  </done>
</task>

<task type="auto">
  <name>Task 3: Add integration tests with real cache and tracker</name>
  <files>tests/integration/test_embedder_integration.py</files>
  <action>
Create integration tests that verify cache and tracker work with real filesystem:

```python
"""Integration tests for OpenAIEmbedder with cache and token tracking."""

from __future__ import annotations

from pathlib import Path
from unittest.mock import AsyncMock, MagicMock

import pytest

from knowledge_mcp.embed.cache import EmbeddingCache
from knowledge_mcp.embed.openai_embedder import OpenAIEmbedder
from knowledge_mcp.monitoring.token_tracker import TokenTracker


class TestEmbedderCacheIntegration:
    """Integration tests with real cache and tracker."""

    @pytest.fixture
    def temp_cache_dir(self, tmp_path: Path) -> Path:
        """Isolated cache directory."""
        cache_dir = tmp_path / "cache"
        cache_dir.mkdir()
        return cache_dir

    @pytest.fixture
    def temp_log_file(self, tmp_path: Path) -> Path:
        """Isolated token log file."""
        return tmp_path / "tokens.json"

    @pytest.fixture
    def real_cache(self, temp_cache_dir: Path) -> EmbeddingCache:
        """Real EmbeddingCache with isolated directory."""
        return EmbeddingCache(temp_cache_dir, "text-embedding-3-small")

    @pytest.fixture
    def real_tracker(self, temp_log_file: Path) -> TokenTracker:
        """Real TokenTracker with isolated log file."""
        return TokenTracker(temp_log_file, "text-embedding-3-small")

    @pytest.fixture
    def embedder_with_real_deps(
        self,
        real_cache: EmbeddingCache,
        real_tracker: TokenTracker,
    ) -> OpenAIEmbedder:
        """Embedder with real cache/tracker but mocked API."""
        embedder = OpenAIEmbedder(
            api_key="sk-test-key",
            cache=real_cache,
            token_tracker=real_tracker,
        )
        # Mock the API call
        embedder._client = MagicMock()
        embedder._client.embeddings = MagicMock()
        embedder._client.embeddings.create = AsyncMock(
            return_value=MagicMock(
                data=[MagicMock(embedding=[0.1] * 1536)]
            )
        )
        return embedder

    @pytest.mark.asyncio
    async def test_cache_hit_prevents_api_call(
        self,
        embedder_with_real_deps: OpenAIEmbedder,
    ) -> None:
        """Verify second call with same text does not call API."""
        # First call - cache miss, API called
        await embedder_with_real_deps.embed("test text")
        first_call_count = embedder_with_real_deps._client.embeddings.create.call_count

        # Second call - cache hit, no API call
        await embedder_with_real_deps.embed("test text")
        second_call_count = embedder_with_real_deps._client.embeddings.create.call_count

        assert first_call_count == 1
        assert second_call_count == 1  # No additional call

    @pytest.mark.asyncio
    async def test_cache_returns_same_embedding(
        self,
        embedder_with_real_deps: OpenAIEmbedder,
    ) -> None:
        """Verify cached embedding matches original."""
        embedding1 = await embedder_with_real_deps.embed("test text")
        embedding2 = await embedder_with_real_deps.embed("test text")

        assert embedding1 == embedding2

    @pytest.mark.asyncio
    async def test_tracker_records_cache_hit(
        self,
        embedder_with_real_deps: OpenAIEmbedder,
        real_tracker: TokenTracker,
    ) -> None:
        """Verify tracker records cache hits correctly."""
        await embedder_with_real_deps.embed("test text")  # Cache miss
        await embedder_with_real_deps.embed("test text")  # Cache hit

        summary = real_tracker.get_daily_summary()
        assert summary.get("cache_hits", 0) == 1
        assert summary.get("embedding_requests", 0) == 1

    @pytest.mark.asyncio
    async def test_cache_persists_across_instances(
        self,
        temp_cache_dir: Path,
        temp_log_file: Path,
    ) -> None:
        """Verify cache persists after embedder is recreated."""
        # First embedder instance
        cache1 = EmbeddingCache(temp_cache_dir, "text-embedding-3-small")
        tracker1 = TokenTracker(temp_log_file, "text-embedding-3-small")
        embedder1 = OpenAIEmbedder(
            api_key="sk-test-key",
            cache=cache1,
            token_tracker=tracker1,
        )
        embedder1._client = MagicMock()
        embedder1._client.embeddings = MagicMock()
        embedder1._client.embeddings.create = AsyncMock(
            return_value=MagicMock(
                data=[MagicMock(embedding=[0.1] * 1536)]
            )
        )

        # Store embedding
        await embedder1.embed("persistent text")
        cache1.close()  # Close cache cleanly

        # Second embedder instance with fresh cache
        cache2 = EmbeddingCache(temp_cache_dir, "text-embedding-3-small")
        tracker2 = TokenTracker(temp_log_file, "text-embedding-3-small")
        embedder2 = OpenAIEmbedder(
            api_key="sk-test-key",
            cache=cache2,
            token_tracker=tracker2,
        )
        embedder2._client = MagicMock()
        embedder2._client.embeddings = MagicMock()
        embedder2._client.embeddings.create = AsyncMock(
            return_value=MagicMock(
                data=[MagicMock(embedding=[0.2] * 1536)]  # Different embedding
            )
        )

        # Should return cached value from first instance
        result = await embedder2.embed("persistent text")

        # Result should be from cache (0.1), not new API call (0.2)
        assert result[0] == 0.1
        embedder2._client.embeddings.create.assert_not_called()
        cache2.close()

    @pytest.mark.asyncio
    async def test_different_texts_not_cached(
        self,
        embedder_with_real_deps: OpenAIEmbedder,
    ) -> None:
        """Verify different texts result in separate API calls."""
        await embedder_with_real_deps.embed("text one")
        await embedder_with_real_deps.embed("text two")

        assert embedder_with_real_deps._client.embeddings.create.call_count == 2
```

Place file at: tests/integration/test_embedder_integration.py
  </action>
  <verify>
```bash
cd /Users/dunnock/projects/claude-plugins/mcps/knowledge-mcp && \
poetry run pytest tests/integration/test_embedder_integration.py -v
```
  </verify>
  <done>
- tests/integration/test_embedder_integration.py exists
- Test verifies cache hit prevents second API call
- Test verifies cached embedding matches original
- Test verifies tracker records cache hits correctly
- Test verifies cache persists across embedder instances
- Test verifies different texts are not cached together
- All integration tests pass
  </done>
</task>

</tasks>

<verification>
```bash
cd /Users/dunnock/projects/claude-plugins/mcps/knowledge-mcp && \
poetry run pytest tests/unit/ tests/integration/ -v && \
poetry run pyright src/knowledge_mcp/server.py
```

Expected: All tests pass, zero pyright errors.

Final verification of phase success criteria:
1. OpenAIEmbedder checks EmbeddingCache before calling OpenAI API
2. Cache hits return immediately without API call
3. OpenAIEmbedder logs token counts via TokenTracker
4. Token data recorded in daily summary
5. All existing tests pass
</verification>

<success_criteria>
1. server.py creates EmbeddingCache when cache_enabled=True
2. server.py creates TokenTracker when token_tracking_enabled=True
3. server.py passes cache and tracker to OpenAIEmbedder
4. Unit tests verify cache check, storage, and token tracking
5. Integration tests verify cache prevents duplicate API calls
6. Integration tests verify cache persists across instances
7. All tests pass (unit and integration)
8. Zero pyright errors in server.py
</success_criteria>

<output>
After completion, create `.planning/phases/05-production-integration/05-02-SUMMARY.md`
</output>
