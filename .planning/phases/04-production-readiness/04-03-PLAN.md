---
phase: 04-production-readiness
plan: 03
type: execute
wave: 2
depends_on: ["04-01"]
files_modified:
  - mcps/knowledge-mcp/src/knowledge_mcp/monitoring/token_tracker.py
  - mcps/knowledge-mcp/src/knowledge_mcp/monitoring/logger.py
  - mcps/knowledge-mcp/src/knowledge_mcp/cli/token_summary.py
  - mcps/knowledge-mcp/tests/unit/test_monitoring.py
autonomous: true

must_haves:
  truths:
    - "Token counts tracked per embedding request"
    - "Daily totals aggregated and logged to JSON file"
    - "CLI command shows token usage summary"
    - "Warning emitted when approaching budget threshold"
  artifacts:
    - path: "mcps/knowledge-mcp/src/knowledge_mcp/monitoring/token_tracker.py"
      provides: "TokenTracker class for cost monitoring"
      exports: ["TokenTracker"]
      min_lines: 80
    - path: "mcps/knowledge-mcp/src/knowledge_mcp/monitoring/logger.py"
      provides: "Structured JSON logging setup"
      exports: ["setup_json_logger"]
    - path: "mcps/knowledge-mcp/src/knowledge_mcp/cli/token_summary.py"
      provides: "CLI command for token usage report"
      min_lines: 40
    - path: "mcps/knowledge-mcp/tests/unit/test_monitoring.py"
      provides: "Unit tests for monitoring module"
      min_lines: 60
  key_links:
    - from: "src/knowledge_mcp/monitoring/token_tracker.py"
      to: "tiktoken"
      via: "token counting"
      pattern: "tiktoken\\.encoding_for_model"
    - from: "src/knowledge_mcp/cli/token_summary.py"
      to: "src/knowledge_mcp/monitoring/token_tracker.py"
      via: "import"
      pattern: "from knowledge_mcp.monitoring.token_tracker import TokenTracker"
---

<objective>
Implement token tracking and cost monitoring with CLI reporting.

Purpose: NFR-5.1 requires logging token counts for cost visibility. This enables tracking API spending and alerting when approaching budget.
Output: TokenTracker class, structured JSON logging, CLI summary command, and unit tests.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@mcps/knowledge-mcp/src/knowledge_mcp/monitoring/__init__.py
@mcps/knowledge-mcp/src/knowledge_mcp/cli/__init__.py
@.planning/phases/04-production-readiness/04-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement TokenTracker and structured logger</name>
  <files>
mcps/knowledge-mcp/src/knowledge_mcp/monitoring/token_tracker.py
mcps/knowledge-mcp/src/knowledge_mcp/monitoring/logger.py
  </files>
  <action>
Create `src/knowledge_mcp/monitoring/token_tracker.py`:

```python
"""Token usage tracking for cost monitoring."""

from __future__ import annotations

import json
from datetime import date
from pathlib import Path
from typing import TYPE_CHECKING

import tiktoken

if TYPE_CHECKING:
    pass


class TokenTracker:
    """
    Track OpenAI API token usage for cost visibility.

    Aggregates daily totals, logs to JSON file.
    Emits warnings when approaching budget threshold.

    Args:
        log_file: Path to JSON file for storing usage stats.
        embedding_model: Model name for tokenizer selection.
        daily_warning_threshold: Token count that triggers warning (default 1M).

    Example:
        >>> tracker = TokenTracker(Path("data/tokens.json"), "text-embedding-3-small")
        >>> tokens = tracker.track_embedding("Hello world", cache_hit=False)
        >>> summary = tracker.get_daily_summary()
        >>> cost = tracker.estimate_cost()
    """

    # OpenAI pricing per 1M tokens (text-embedding-3-small)
    COST_PER_MILLION_TOKENS = 0.020

    def __init__(
        self,
        log_file: Path,
        embedding_model: str = "text-embedding-3-small",
        daily_warning_threshold: int = 1_000_000,
    ) -> None:
        """Initialize tracker with log file and tokenizer."""
        self.log_file = log_file
        self.log_file.parent.mkdir(parents=True, exist_ok=True)
        self.embedding_model = embedding_model
        self.daily_warning_threshold = daily_warning_threshold

        # Initialize tokenizer
        try:
            self.encoding = tiktoken.encoding_for_model(embedding_model)
        except KeyError:
            # Fallback for models not in tiktoken registry
            self.encoding = tiktoken.get_encoding("cl100k_base")

        # Load existing stats
        self.stats: dict[str, dict[str, int]] = self._load_stats()

    def _load_stats(self) -> dict[str, dict[str, int]]:
        """Load existing token stats from JSON."""
        if self.log_file.exists():
            with open(self.log_file, encoding="utf-8") as f:
                return json.load(f)
        return {}

    def _save_stats(self) -> None:
        """Persist stats to JSON."""
        with open(self.log_file, "w", encoding="utf-8") as f:
            json.dump(self.stats, f, indent=2)

    def _get_today(self) -> str:
        """Get today's date as string key."""
        return str(date.today())

    def _ensure_today_entry(self) -> dict[str, int]:
        """Ensure today's entry exists and return it."""
        today = self._get_today()
        if today not in self.stats:
            self.stats[today] = {
                "embedding_tokens": 0,
                "embedding_requests": 0,
                "cache_hits": 0,
            }
        return self.stats[today]

    def count_tokens(self, text: str) -> int:
        """
        Count tokens in text using tiktoken.

        Args:
            text: Text to tokenize.

        Returns:
            Number of tokens.
        """
        return len(self.encoding.encode(text))

    def track_embedding(self, text: str, cache_hit: bool = False) -> int:
        """
        Track embedding token usage.

        Args:
            text: Text being embedded.
            cache_hit: Whether embedding was served from cache.

        Returns:
            Token count for the text.
        """
        tokens = self.count_tokens(text)
        today_stats = self._ensure_today_entry()

        if cache_hit:
            today_stats["cache_hits"] += 1
        else:
            today_stats["embedding_tokens"] += tokens
            today_stats["embedding_requests"] += 1

        self._save_stats()

        # Check warning threshold
        if today_stats["embedding_tokens"] >= self.daily_warning_threshold:
            self._emit_warning(today_stats["embedding_tokens"])

        return tokens

    def _emit_warning(self, tokens: int) -> None:
        """Emit warning when approaching threshold."""
        import logging
        logger = logging.getLogger("knowledge_mcp.monitoring")
        cost = (tokens / 1_000_000) * self.COST_PER_MILLION_TOKENS
        logger.warning(
            f"Daily token usage high: {tokens:,} tokens (${cost:.4f}). "
            f"Threshold: {self.daily_warning_threshold:,}"
        )

    def get_daily_summary(self, day: str | None = None) -> dict[str, int]:
        """
        Get summary for specific day.

        Args:
            day: Date string (YYYY-MM-DD), defaults to today.

        Returns:
            Dict with embedding_tokens, embedding_requests, cache_hits.
        """
        day = day or self._get_today()
        return self.stats.get(day, {})

    def estimate_cost(self, day: str | None = None) -> float:
        """
        Estimate cost for a day.

        Args:
            day: Date string (YYYY-MM-DD), defaults to today.

        Returns:
            Estimated cost in USD.
        """
        summary = self.get_daily_summary(day)
        tokens = summary.get("embedding_tokens", 0)
        return (tokens / 1_000_000) * self.COST_PER_MILLION_TOKENS

    def get_all_days(self) -> list[str]:
        """Get list of all days with recorded usage."""
        return sorted(self.stats.keys(), reverse=True)
```

Create `src/knowledge_mcp/monitoring/logger.py`:

```python
"""Structured JSON logging for production monitoring."""

from __future__ import annotations

import logging
import sys
from typing import TYPE_CHECKING

from pythonjsonlogger import jsonlogger

if TYPE_CHECKING:
    pass


def setup_json_logger(
    name: str,
    level: int = logging.INFO,
) -> logging.Logger:
    """
    Configure structured JSON logger.

    Output schema: timestamp (ISO 8601), level, name, message, plus extra fields.

    Args:
        name: Logger name (typically module path).
        level: Logging level (default INFO).

    Returns:
        Configured logger instance.

    Example:
        >>> logger = setup_json_logger("knowledge_mcp.search")
        >>> logger.info("Search completed", extra={"query": "test", "results": 5})
        # Output: {"timestamp":"2026-01-24T10:30:15","level":"INFO","name":"knowledge_mcp.search","message":"Search completed","query":"test","results":5}
    """
    logger = logging.getLogger(name)
    logger.setLevel(level)

    # Avoid duplicate handlers
    if not logger.handlers:
        handler = logging.StreamHandler(sys.stdout)
        formatter = jsonlogger.JsonFormatter(
            "%(asctime)s %(levelname)s %(name)s %(message)s",
            rename_fields={"asctime": "timestamp", "levelname": "level"},
            timestamp=True,
        )
        handler.setFormatter(formatter)
        logger.addHandler(handler)

    return logger
```

Update `src/knowledge_mcp/monitoring/__init__.py`:
```python
"""
Production monitoring for knowledge-mcp.

This package provides:
- Token usage tracking for cost visibility
- Structured JSON logging
- Alerting for threshold violations
"""

from __future__ import annotations

from knowledge_mcp.monitoring.logger import setup_json_logger
from knowledge_mcp.monitoring.token_tracker import TokenTracker

__all__ = ["TokenTracker", "setup_json_logger"]
```
  </action>
  <verify>`poetry run python -c "from knowledge_mcp.monitoring import TokenTracker, setup_json_logger; print('OK')"` succeeds</verify>
  <done>TokenTracker and setup_json_logger implemented with daily aggregation and warnings</done>
</task>

<task type="auto">
  <name>Task 2: Create CLI token summary command</name>
  <files>mcps/knowledge-mcp/src/knowledge_mcp/cli/token_summary.py</files>
  <action>
Create `src/knowledge_mcp/cli/token_summary.py`:

```python
"""CLI command for token usage summary."""

from __future__ import annotations

from datetime import date, timedelta
from pathlib import Path
from typing import TYPE_CHECKING

from rich.console import Console
from rich.table import Table

from knowledge_mcp.monitoring.token_tracker import TokenTracker

if TYPE_CHECKING:
    pass


def token_summary_command(
    log_file: Path | None = None,
    days: int = 7,
    weekly_budget: float = 10.0,
) -> None:
    """
    Display token usage and cost summary.

    Args:
        log_file: Path to token usage log (default: data/token_usage.json).
        days: Number of days to display.
        weekly_budget: Weekly budget in USD for warning threshold.
    """
    log_file = log_file or Path("data/token_usage.json")
    console = Console()

    if not log_file.exists():
        console.print("[yellow]No token usage data found.[/yellow]")
        console.print(f"Expected file: {log_file}")
        return

    tracker = TokenTracker(log_file)

    # Create summary table
    table = Table(title=f"Token Usage (Last {days} Days)")
    table.add_column("Date", style="cyan")
    table.add_column("Requests", justify="right")
    table.add_column("Tokens", justify="right")
    table.add_column("Cache Hits", justify="right", style="green")
    table.add_column("Cost (USD)", justify="right", style="yellow")

    total_cost = 0.0
    total_tokens = 0
    total_cache_hits = 0

    for i in range(days):
        day = str(date.today() - timedelta(days=i))
        summary = tracker.get_daily_summary(day)

        if summary:
            cost = tracker.estimate_cost(day)
            total_cost += cost
            total_tokens += summary.get("embedding_tokens", 0)
            total_cache_hits += summary.get("cache_hits", 0)

            table.add_row(
                day,
                str(summary.get("embedding_requests", 0)),
                f"{summary.get('embedding_tokens', 0):,}",
                str(summary.get("cache_hits", 0)),
                f"${cost:.4f}",
            )

    console.print(table)
    console.print()
    console.print(f"[bold]Total Cost ({days} days):[/bold] ${total_cost:.4f}")
    console.print(f"[bold]Total Tokens:[/bold] {total_tokens:,}")
    console.print(f"[bold]Total Cache Hits:[/bold] {total_cache_hits:,}")

    if total_tokens > 0:
        cache_hit_rate = total_cache_hits / (total_cache_hits + total_tokens / 100) * 100
        console.print(f"[bold]Cache Efficiency:[/bold] ~{cache_hit_rate:.1f}%")

    # Warn if approaching budget
    if total_cost > weekly_budget * 0.8:
        console.print(
            f"\n[yellow]Warning: Approaching weekly budget (${weekly_budget:.2f})[/yellow]"
        )


def main() -> None:
    """Entry point for CLI command."""
    import argparse

    parser = argparse.ArgumentParser(description="Token usage summary")
    parser.add_argument(
        "--log-file",
        type=Path,
        default=None,
        help="Path to token usage log",
    )
    parser.add_argument(
        "--days",
        type=int,
        default=7,
        help="Number of days to display",
    )
    parser.add_argument(
        "--budget",
        type=float,
        default=10.0,
        help="Weekly budget in USD",
    )

    args = parser.parse_args()
    token_summary_command(args.log_file, args.days, args.budget)


if __name__ == "__main__":
    main()
```

Update `src/knowledge_mcp/cli/__init__.py` to export the command:
```python
"""CLI commands for knowledge-mcp."""

from __future__ import annotations

__all__: list[str] = []
```

Test CLI runs:
```bash
poetry run python -m knowledge_mcp.cli.token_summary --help
```
  </action>
  <verify>`poetry run python -m knowledge_mcp.cli.token_summary --help` shows usage</verify>
  <done>CLI command shows token usage table with Rich formatting</done>
</task>

<task type="auto">
  <name>Task 3: Create unit tests for monitoring module</name>
  <files>mcps/knowledge-mcp/tests/unit/test_monitoring.py</files>
  <action>
Create `tests/unit/test_monitoring.py`:

```python
"""Unit tests for monitoring module."""

from __future__ import annotations

import json
import tempfile
from pathlib import Path

import pytest

from knowledge_mcp.monitoring.token_tracker import TokenTracker
from knowledge_mcp.monitoring.logger import setup_json_logger


class TestTokenTracker:
    """Tests for TokenTracker class."""

    @pytest.fixture
    def log_file(self) -> Path:
        """Create temporary log file."""
        with tempfile.NamedTemporaryFile(suffix=".json", delete=False) as f:
            yield Path(f.name)

    @pytest.fixture
    def tracker(self, log_file: Path) -> TokenTracker:
        """Create tracker instance for testing."""
        return TokenTracker(log_file)

    def test_count_tokens(self, tracker: TokenTracker) -> None:
        """Test token counting with tiktoken."""
        # "Hello world" is typically 2 tokens
        count = tracker.count_tokens("Hello world")
        assert count > 0
        assert isinstance(count, int)

    def test_track_embedding_increments_counters(self, tracker: TokenTracker) -> None:
        """Test that tracking increments daily counters."""
        tracker.track_embedding("First text")
        tracker.track_embedding("Second text")

        summary = tracker.get_daily_summary()
        assert summary["embedding_requests"] == 2
        assert summary["embedding_tokens"] > 0

    def test_track_cache_hit_increments_cache_counter(self, tracker: TokenTracker) -> None:
        """Test that cache hits are tracked separately."""
        tracker.track_embedding("Cached text", cache_hit=True)
        tracker.track_embedding("API text", cache_hit=False)

        summary = tracker.get_daily_summary()
        assert summary["cache_hits"] == 1
        assert summary["embedding_requests"] == 1

    def test_estimate_cost(self, tracker: TokenTracker) -> None:
        """Test cost estimation."""
        # Track enough tokens to have measurable cost
        for _ in range(100):
            tracker.track_embedding("Test text for cost estimation")

        cost = tracker.estimate_cost()
        assert cost > 0
        assert cost < 1.0  # Should be small for test data

    def test_persistence(self, log_file: Path) -> None:
        """Test that stats persist across instances."""
        tracker1 = TokenTracker(log_file)
        tracker1.track_embedding("Persistent text")
        tokens_1 = tracker1.get_daily_summary()["embedding_tokens"]

        tracker2 = TokenTracker(log_file)
        tokens_2 = tracker2.get_daily_summary()["embedding_tokens"]

        assert tokens_2 == tokens_1

    def test_get_all_days(self, tracker: TokenTracker) -> None:
        """Test get_all_days returns tracked days."""
        tracker.track_embedding("Some text")
        days = tracker.get_all_days()

        assert len(days) >= 1
        assert all(isinstance(d, str) for d in days)

    def test_empty_summary_for_missing_day(self, tracker: TokenTracker) -> None:
        """Test that missing day returns empty dict."""
        summary = tracker.get_daily_summary("1900-01-01")
        assert summary == {}


class TestSetupJsonLogger:
    """Tests for structured JSON logger."""

    def test_logger_returns_logger_instance(self) -> None:
        """Test that setup returns a logger."""
        import logging
        logger = setup_json_logger("test.module")
        assert isinstance(logger, logging.Logger)

    def test_logger_name_set(self) -> None:
        """Test that logger has correct name."""
        logger = setup_json_logger("knowledge_mcp.test")
        assert logger.name == "knowledge_mcp.test"

    def test_logger_level_configurable(self) -> None:
        """Test that logging level is configurable."""
        import logging
        logger = setup_json_logger("test.level", level=logging.DEBUG)
        assert logger.level == logging.DEBUG

    def test_no_duplicate_handlers(self) -> None:
        """Test that multiple calls don't add duplicate handlers."""
        logger1 = setup_json_logger("test.duplicate")
        handler_count_1 = len(logger1.handlers)

        logger2 = setup_json_logger("test.duplicate")
        handler_count_2 = len(logger2.handlers)

        assert handler_count_1 == handler_count_2
```

Run tests:
```bash
cd mcps/knowledge-mcp && poetry run pytest tests/unit/test_monitoring.py -v
```
  </action>
  <verify>`poetry run pytest tests/unit/test_monitoring.py -v` shows all tests passing</verify>
  <done>12+ unit tests covering TokenTracker and logger functionality</done>
</task>

</tasks>

<verification>
Run verification sequence:
```bash
cd mcps/knowledge-mcp

# 1. Import test
poetry run python -c "from knowledge_mcp.monitoring import TokenTracker, setup_json_logger; print('Import: OK')"

# 2. Token counting test
poetry run python -c "
from knowledge_mcp.monitoring import TokenTracker
from pathlib import Path
import tempfile

with tempfile.NamedTemporaryFile(suffix='.json') as f:
    t = TokenTracker(Path(f.name))
    tokens = t.count_tokens('Hello world')
    print(f'Token count: {tokens}')
    print('TokenTracker: OK')
"

# 3. CLI help
poetry run python -m knowledge_mcp.cli.token_summary --help

# 4. Run unit tests
poetry run pytest tests/unit/test_monitoring.py -v

# 5. Run all tests (no regressions)
poetry run pytest tests/ -v --tb=short
```
</verification>

<success_criteria>
- [ ] TokenTracker class with daily aggregation to JSON
- [ ] Token counting via tiktoken
- [ ] Warning emission when exceeding threshold
- [ ] setup_json_logger for structured logging
- [ ] CLI command with Rich table output
- [ ] 12+ unit tests passing
- [ ] No regressions in existing tests
</success_criteria>

<output>
After completion, create `.planning/phases/04-production-readiness/04-03-SUMMARY.md`
</output>
