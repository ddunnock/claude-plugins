---
phase: 04-production-readiness
plan: 04
type: execute
wave: 3
depends_on: ["04-02", "04-03"]
files_modified:
  - mcps/knowledge-mcp/src/knowledge_mcp/evaluation/metrics.py
  - mcps/knowledge-mcp/src/knowledge_mcp/evaluation/golden_set.py
  - mcps/knowledge-mcp/src/knowledge_mcp/evaluation/reporter.py
  - mcps/knowledge-mcp/data/golden_queries.yml
  - mcps/knowledge-mcp/tests/evaluation/test_golden_set.py
autonomous: true

must_haves:
  truths:
    - "Golden query file exists with 30+ test queries"
    - "RAG metrics can be computed for search results"
    - "CLI reporter shows pass/fail status with threshold checks"
    - "Golden test runner validates search recall"
  artifacts:
    - path: "mcps/knowledge-mcp/src/knowledge_mcp/evaluation/metrics.py"
      provides: "RAG Triad metrics wrapper"
      exports: ["evaluate_rag_triad", "evaluate_retrieval_only"]
      min_lines: 50
    - path: "mcps/knowledge-mcp/src/knowledge_mcp/evaluation/golden_set.py"
      provides: "Golden test set loader and runner"
      exports: ["GoldenTestSet", "run_golden_tests"]
      min_lines: 60
    - path: "mcps/knowledge-mcp/src/knowledge_mcp/evaluation/reporter.py"
      provides: "CLI evaluation reporter"
      exports: ["print_evaluation_summary", "print_golden_results"]
      min_lines: 40
    - path: "mcps/knowledge-mcp/data/golden_queries.yml"
      provides: "Golden test queries for evaluation"
      min_lines: 100
  key_links:
    - from: "src/knowledge_mcp/evaluation/metrics.py"
      to: "ragas"
      via: "import and evaluate"
      pattern: "from ragas"
    - from: "src/knowledge_mcp/evaluation/golden_set.py"
      to: "data/golden_queries.yml"
      via: "YAML loading"
      pattern: "yaml\\.safe_load"
---

<objective>
Implement golden test set and RAG evaluation framework for systematic quality assessment.

Purpose: NFR-2.1 requires a golden test set (20-50 queries), NFR-2.2 requires RAG Triad metrics tracking. This enables continuous evaluation on every PR.
Output: Golden query file, RAGAS integration for metrics, test runner, CLI reporter.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@mcps/knowledge-mcp/src/knowledge_mcp/evaluation/__init__.py
@mcps/knowledge-mcp/src/knowledge_mcp/search/semantic_search.py
@.planning/phases/04-production-readiness/04-RESEARCH.md
@.planning/phases/04-production-readiness/04-CONTEXT.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement RAG metrics wrapper</name>
  <files>mcps/knowledge-mcp/src/knowledge_mcp/evaluation/metrics.py</files>
  <action>
Create `src/knowledge_mcp/evaluation/metrics.py`:

```python
"""RAG evaluation metrics wrapper using RAGAS."""

from __future__ import annotations

from typing import TYPE_CHECKING

if TYPE_CHECKING:
    pass


def evaluate_retrieval_only(
    queries: list[str],
    retrieved_contexts: list[list[str]],
    expected_in_results: list[list[str]],
) -> dict[str, float]:
    """
    Evaluate retrieval quality without LLM generation.

    This is a lightweight evaluation that doesn't require LLM calls.
    Measures recall@k: what fraction of expected results appear in retrieved.

    Args:
        queries: User queries.
        retrieved_contexts: Retrieved document chunks per query.
        expected_in_results: Expected content/clauses that should appear.

    Returns:
        Dict with recall_at_k scores.
    """
    recalls = []
    for expected, retrieved in zip(expected_in_results, retrieved_contexts, strict=True):
        if not expected:
            recalls.append(1.0)  # No expectations = pass
            continue

        # Count how many expected items appear in retrieved
        hits = sum(
            1 for exp in expected
            if any(exp.lower() in ctx.lower() for ctx in retrieved)
        )
        recall = hits / len(expected) if expected else 0.0
        recalls.append(recall)

    return {
        "recall_at_k": sum(recalls) / len(recalls) if recalls else 0.0,
        "queries_evaluated": len(queries),
        "queries_passing": sum(1 for r in recalls if r >= 0.8),
    }


def evaluate_rag_triad(
    queries: list[str],
    retrieved_contexts: list[list[str]],
    ground_truths: list[str],
    answers: list[str],
) -> dict[str, float]:
    """
    Evaluate RAG system using RAG Triad metrics via RAGAS.

    Note: This requires LLM calls and is more expensive. Use for
    periodic deep evaluation, not every test run.

    Args:
        queries: User queries.
        retrieved_contexts: Retrieved document chunks per query.
        ground_truths: Expected answers (for context recall).
        answers: Generated answers (for faithfulness check).

    Returns:
        Dict with context_precision, faithfulness, answer_relevancy scores.

    Raises:
        ImportError: If ragas is not installed.
    """
    try:
        from ragas import evaluate
        from ragas.metrics import (
            answer_relevancy,
            context_precision,
            faithfulness,
        )
        from datasets import Dataset
    except ImportError as e:
        raise ImportError(
            "ragas and datasets required for RAG Triad evaluation. "
            "Install with: poetry install --with dev"
        ) from e

    dataset = Dataset.from_dict({
        "question": queries,
        "contexts": retrieved_contexts,
        "ground_truth": ground_truths,
        "answer": answers,
    })

    result = evaluate(
        dataset,
        metrics=[
            context_precision,
            faithfulness,
            answer_relevancy,
        ],
    )

    # Extract scores from result
    scores = {
        "context_precision": float(result["context_precision"]),
        "faithfulness": float(result["faithfulness"]),
        "answer_relevancy": float(result["answer_relevancy"]),
    }

    # Calculate overall score
    scores["overall_score"] = sum(scores.values()) / len(scores)

    return scores
```
  </action>
  <verify>`poetry run python -c "from knowledge_mcp.evaluation.metrics import evaluate_retrieval_only; print('OK')"` succeeds</verify>
  <done>RAG metrics functions implemented with lightweight retrieval-only option</done>
</task>

<task type="auto">
  <name>Task 2: Create golden test set loader and initial queries</name>
  <files>
mcps/knowledge-mcp/src/knowledge_mcp/evaluation/golden_set.py
mcps/knowledge-mcp/data/golden_queries.yml
  </files>
  <action>
Create `src/knowledge_mcp/evaluation/golden_set.py`:

```python
"""Golden test set management for RAG evaluation."""

from __future__ import annotations

from dataclasses import dataclass, field
from pathlib import Path
from typing import TYPE_CHECKING

import yaml

if TYPE_CHECKING:
    pass


@dataclass
class GoldenQuery:
    """Single golden test query."""

    query: str
    expected_in_top_k: list[str]
    category: str = "general"
    difficulty: str = "medium"
    sources: list[str] = field(default_factory=list)
    notes: str = ""


@dataclass
class GoldenTestResult:
    """Result of a single golden test."""

    query: str
    expected: list[str]
    retrieved: list[str]
    recall: float
    passed: bool


class GoldenTestSet:
    """
    Manages golden test queries for RAG evaluation.

    Loads queries from YAML file, runs against search engine,
    calculates recall@k metrics.

    Args:
        queries_file: Path to YAML file with golden queries.
        k: Number of results to check (top-k).
        pass_threshold: Minimum recall to pass (default 0.8).

    Example:
        >>> golden = GoldenTestSet(Path("data/golden_queries.yml"))
        >>> queries = golden.load_queries()
        >>> results = golden.evaluate(searcher)
    """

    def __init__(
        self,
        queries_file: Path,
        k: int = 5,
        pass_threshold: float = 0.8,
    ) -> None:
        """Initialize golden test set."""
        self.queries_file = queries_file
        self.k = k
        self.pass_threshold = pass_threshold
        self._queries: list[GoldenQuery] | None = None

    def load_queries(self) -> list[GoldenQuery]:
        """
        Load golden queries from YAML file.

        Returns:
            List of GoldenQuery objects.

        Raises:
            FileNotFoundError: If queries file doesn't exist.
        """
        if self._queries is not None:
            return self._queries

        with open(self.queries_file, encoding="utf-8") as f:
            data = yaml.safe_load(f)

        self._queries = [
            GoldenQuery(
                query=q["query"],
                expected_in_top_k=q.get("expected_in_top_k", []),
                category=q.get("category", "general"),
                difficulty=q.get("difficulty", "medium"),
                sources=q.get("sources", []),
                notes=q.get("notes", ""),
            )
            for q in data.get("queries", [])
        ]

        return self._queries

    def evaluate_single(
        self,
        golden: GoldenQuery,
        retrieved_content: list[str],
    ) -> GoldenTestResult:
        """
        Evaluate single query against retrieved results.

        Args:
            golden: Golden query with expectations.
            retrieved_content: Retrieved text chunks.

        Returns:
            GoldenTestResult with pass/fail status.
        """
        # Check how many expected items appear in retrieved
        hits = sum(
            1 for exp in golden.expected_in_top_k
            if any(exp.lower() in ctx.lower() for ctx in retrieved_content)
        )

        recall = hits / len(golden.expected_in_top_k) if golden.expected_in_top_k else 1.0
        passed = recall >= self.pass_threshold

        return GoldenTestResult(
            query=golden.query,
            expected=golden.expected_in_top_k,
            retrieved=retrieved_content[:self.k],
            recall=recall,
            passed=passed,
        )

    def get_summary(self, results: list[GoldenTestResult]) -> dict[str, float | int]:
        """
        Get summary statistics from test results.

        Args:
            results: List of test results.

        Returns:
            Dict with pass_rate, avg_recall, total, passed, failed.
        """
        if not results:
            return {
                "pass_rate": 0.0,
                "avg_recall": 0.0,
                "total": 0,
                "passed": 0,
                "failed": 0,
            }

        passed = sum(1 for r in results if r.passed)
        avg_recall = sum(r.recall for r in results) / len(results)

        return {
            "pass_rate": passed / len(results),
            "avg_recall": avg_recall,
            "total": len(results),
            "passed": passed,
            "failed": len(results) - passed,
        }


def run_golden_tests(
    golden_file: Path,
    search_fn: callable,
    k: int = 5,
) -> tuple[list[GoldenTestResult], dict[str, float | int]]:
    """
    Run golden tests against a search function.

    Args:
        golden_file: Path to golden queries YAML.
        search_fn: Function that takes query string, returns list of result dicts
                   with 'content' key.
        k: Number of results to retrieve.

    Returns:
        Tuple of (results list, summary dict).
    """
    golden_set = GoldenTestSet(golden_file, k=k)
    queries = golden_set.load_queries()

    results = []
    for query in queries:
        # Run search
        search_results = search_fn(query.query)

        # Extract content from results
        retrieved_content = [
            r.get("content", str(r))
            for r in search_results[:k]
        ]

        # Evaluate
        result = golden_set.evaluate_single(query, retrieved_content)
        results.append(result)

    summary = golden_set.get_summary(results)
    return results, summary
```

Create `data/golden_queries.yml` with initial queries (30+ queries covering standards):

```yaml
# Golden Test Set for Knowledge MCP
#
# Query categories:
# - verification: System verification and validation
# - requirements: Requirements engineering
# - architecture: System architecture
# - safety: Functional safety
# - process: Development processes
# - definitions: Terminology lookups
#
# Difficulty levels:
# - easy: Single-fact lookup
# - medium: Cross-standard or multi-concept
# - hard: Multi-hop reasoning or synthesis

queries:
  # ===== VERIFICATION (8 queries) =====
  - query: "What are the requirements for system verification?"
    expected_in_top_k:
      - "verification"
      - "requirements"
    category: verification
    difficulty: easy
    sources: ["IEEE 15288", "ISO 15288"]

  - query: "When should verification activities be performed?"
    expected_in_top_k:
      - "verification"
      - "life cycle"
    category: verification
    difficulty: medium
    sources: ["IEEE 15288"]

  - query: "What is the difference between verification and validation?"
    expected_in_top_k:
      - "verification"
      - "validation"
    category: verification
    difficulty: easy
    sources: ["INCOSE Handbook"]

  - query: "What test methods are recommended for verification?"
    expected_in_top_k:
      - "test"
      - "verification"
    category: verification
    difficulty: medium
    sources: ["IEEE 15288", "INCOSE"]

  - query: "How do you verify non-functional requirements?"
    expected_in_top_k:
      - "non-functional"
      - "verification"
    category: verification
    difficulty: hard
    sources: ["INCOSE Handbook"]

  - query: "What artifacts are produced by verification process?"
    expected_in_top_k:
      - "verification"
      - "artifact"
    category: verification
    difficulty: medium
    sources: ["IEEE 15288"]

  - query: "Verification traceability requirements"
    expected_in_top_k:
      - "traceability"
      - "verification"
    category: verification
    difficulty: medium
    sources: ["IEEE 15288"]

  - query: "Independent verification and validation requirements"
    expected_in_top_k:
      - "independent"
      - "verification"
    category: verification
    difficulty: hard
    sources: ["NASA", "IEEE"]

  # ===== REQUIREMENTS (7 queries) =====
  - query: "How should requirements be documented?"
    expected_in_top_k:
      - "requirements"
      - "documentation"
    category: requirements
    difficulty: easy
    sources: ["IEEE 29148"]

  - query: "What makes a requirement well-formed?"
    expected_in_top_k:
      - "requirement"
      - "shall"
    category: requirements
    difficulty: easy
    sources: ["INCOSE Handbook"]

  - query: "Requirements traceability matrix purpose"
    expected_in_top_k:
      - "traceability"
      - "requirements"
    category: requirements
    difficulty: medium
    sources: ["IEEE 29148"]

  - query: "Functional vs non-functional requirements"
    expected_in_top_k:
      - "functional"
      - "requirements"
    category: requirements
    difficulty: easy
    sources: ["INCOSE Handbook"]

  - query: "Requirements elicitation techniques"
    expected_in_top_k:
      - "elicitation"
      - "requirements"
    category: requirements
    difficulty: medium
    sources: ["INCOSE Handbook"]

  - query: "How to manage requirements changes?"
    expected_in_top_k:
      - "change"
      - "requirements"
    category: requirements
    difficulty: medium
    sources: ["IEEE 15288"]

  - query: "Stakeholder requirements definition process"
    expected_in_top_k:
      - "stakeholder"
      - "requirements"
    category: requirements
    difficulty: medium
    sources: ["IEEE 15288"]

  # ===== ARCHITECTURE (6 queries) =====
  - query: "What is system architecture?"
    expected_in_top_k:
      - "architecture"
      - "system"
    category: architecture
    difficulty: easy
    sources: ["INCOSE Handbook"]

  - query: "Architecture design principles"
    expected_in_top_k:
      - "architecture"
      - "design"
    category: architecture
    difficulty: medium
    sources: ["INCOSE Handbook"]

  - query: "How to document system architecture?"
    expected_in_top_k:
      - "architecture"
      - "documentation"
    category: architecture
    difficulty: medium
    sources: ["IEEE 42010"]

  - query: "Architecture viewpoints and views"
    expected_in_top_k:
      - "viewpoint"
      - "architecture"
    category: architecture
    difficulty: medium
    sources: ["IEEE 42010"]

  - query: "Interface definition in system architecture"
    expected_in_top_k:
      - "interface"
      - "architecture"
    category: architecture
    difficulty: medium
    sources: ["IEEE 15288"]

  - query: "Architecture evaluation methods"
    expected_in_top_k:
      - "architecture"
      - "evaluation"
    category: architecture
    difficulty: hard
    sources: ["INCOSE Handbook"]

  # ===== SAFETY (5 queries) =====
  - query: "What is ASIL in functional safety?"
    expected_in_top_k:
      - "ASIL"
      - "safety"
    category: safety
    difficulty: easy
    sources: ["ISO 26262"]
    notes: "Automotive Safety Integrity Level"

  - query: "Hazard analysis methods"
    expected_in_top_k:
      - "hazard"
      - "analysis"
    category: safety
    difficulty: medium
    sources: ["ISO 26262", "INCOSE"]

  - query: "Safety requirements derivation"
    expected_in_top_k:
      - "safety"
      - "requirements"
    category: safety
    difficulty: medium
    sources: ["ISO 26262"]

  - query: "Fault tree analysis process"
    expected_in_top_k:
      - "fault tree"
      - "analysis"
    category: safety
    difficulty: medium
    sources: ["ISO 26262"]

  - query: "Safety validation requirements"
    expected_in_top_k:
      - "safety"
      - "validation"
    category: safety
    difficulty: hard
    sources: ["ISO 26262"]

  # ===== PROCESS (4 queries) =====
  - query: "Systems engineering life cycle stages"
    expected_in_top_k:
      - "life cycle"
      - "stage"
    category: process
    difficulty: easy
    sources: ["IEEE 15288"]

  - query: "What is configuration management?"
    expected_in_top_k:
      - "configuration"
      - "management"
    category: process
    difficulty: easy
    sources: ["IEEE 15288"]

  - query: "Technical reviews in systems engineering"
    expected_in_top_k:
      - "review"
      - "technical"
    category: process
    difficulty: medium
    sources: ["IEEE 15288"]

  - query: "Risk management process steps"
    expected_in_top_k:
      - "risk"
      - "management"
    category: process
    difficulty: medium
    sources: ["IEEE 15288", "INCOSE"]

  # ===== DEFINITIONS (4 queries) =====
  - query: "Define system of systems"
    expected_in_top_k:
      - "system of systems"
    category: definitions
    difficulty: easy
    sources: ["INCOSE Handbook"]

  - query: "What is a baseline in systems engineering?"
    expected_in_top_k:
      - "baseline"
    category: definitions
    difficulty: easy
    sources: ["IEEE 15288"]

  - query: "Definition of system element"
    expected_in_top_k:
      - "element"
      - "system"
    category: definitions
    difficulty: easy
    sources: ["IEEE 15288"]

  - query: "What is technical debt?"
    expected_in_top_k:
      - "technical debt"
    category: definitions
    difficulty: medium
    sources: ["INCOSE Handbook"]
```

Ensure data/ directory exists:
```bash
mkdir -p mcps/knowledge-mcp/data
```
  </action>
  <verify>`poetry run python -c "from knowledge_mcp.evaluation.golden_set import GoldenTestSet; print('OK')"` succeeds and data/golden_queries.yml exists</verify>
  <done>Golden test loader and 34 initial queries across 6 categories</done>
</task>

<task type="auto">
  <name>Task 3: Create evaluation reporter and tests</name>
  <files>
mcps/knowledge-mcp/src/knowledge_mcp/evaluation/reporter.py
mcps/knowledge-mcp/tests/evaluation/test_golden_set.py
mcps/knowledge-mcp/src/knowledge_mcp/evaluation/__init__.py
  </files>
  <action>
Create `src/knowledge_mcp/evaluation/reporter.py`:

```python
"""CLI reporting for evaluation results."""

from __future__ import annotations

from typing import TYPE_CHECKING

from rich.console import Console
from rich.table import Table

if TYPE_CHECKING:
    from knowledge_mcp.evaluation.golden_set import GoldenTestResult


def print_evaluation_summary(results: dict[str, float], threshold: float = 0.8) -> None:
    """
    Print formatted RAG evaluation summary to terminal.

    Args:
        results: Dict with metric names and scores.
        threshold: Pass threshold for status display.
    """
    console = Console()

    # RAG metrics table
    table = Table(title="RAG Evaluation Metrics")
    table.add_column("Metric", style="cyan")
    table.add_column("Score", justify="right", style="green")
    table.add_column("Status", justify="center")

    for metric, score in results.items():
        if isinstance(score, (int, float)):
            status = "[green]PASS[/green]" if score >= threshold else "[red]FAIL[/red]"
            table.add_row(
                metric.replace("_", " ").title(),
                f"{score:.2%}" if score <= 1.0 else str(score),
                status,
            )

    console.print(table)

    # Warning if any metric below threshold
    failed = [m for m, s in results.items() if isinstance(s, float) and s < threshold]
    if failed:
        console.print(
            f"\n[bold red]WARNING: {len(failed)} metric(s) below {threshold:.0%} threshold[/bold red]"
        )


def print_golden_results(
    results: list[GoldenTestResult],
    summary: dict[str, float | int],
    verbose: bool = False,
) -> None:
    """
    Print golden test results to terminal.

    Args:
        results: List of test results.
        summary: Summary statistics.
        verbose: Whether to show individual test details.
    """
    console = Console()

    # Summary table
    summary_table = Table(title="Golden Test Summary")
    summary_table.add_column("Metric", style="cyan")
    summary_table.add_column("Value", justify="right")

    summary_table.add_row("Total Tests", str(summary["total"]))
    summary_table.add_row("Passed", f"[green]{summary['passed']}[/green]")
    summary_table.add_row("Failed", f"[red]{summary['failed']}[/red]")
    summary_table.add_row("Pass Rate", f"{summary['pass_rate']:.1%}")
    summary_table.add_row("Avg Recall@k", f"{summary['avg_recall']:.2%}")

    console.print(summary_table)

    # Verbose: show all results
    if verbose:
        console.print("\n[bold]Individual Test Results:[/bold]\n")

        details_table = Table()
        details_table.add_column("Query", max_width=50)
        details_table.add_column("Recall", justify="right")
        details_table.add_column("Status", justify="center")

        for result in results:
            status = "[green]PASS[/green]" if result.passed else "[red]FAIL[/red]"
            query_short = result.query[:47] + "..." if len(result.query) > 50 else result.query
            details_table.add_row(
                query_short,
                f"{result.recall:.0%}",
                status,
            )

        console.print(details_table)

    # Show failing tests
    failed = [r for r in results if not r.passed]
    if failed and not verbose:
        console.print(f"\n[yellow]Failing tests ({len(failed)}):[/yellow]")
        for r in failed[:5]:  # Show first 5
            console.print(f"  - {r.query[:60]}...")
        if len(failed) > 5:
            console.print(f"  ... and {len(failed) - 5} more")

    # Overall status
    if summary["pass_rate"] >= 0.8:
        console.print("\n[bold green]EVALUATION PASSED[/bold green]")
    else:
        console.print("\n[bold red]EVALUATION FAILED[/bold red]")
```

Create `tests/evaluation/test_golden_set.py`:

```python
"""Tests for golden test set functionality."""

from __future__ import annotations

import tempfile
from pathlib import Path

import pytest
import yaml

from knowledge_mcp.evaluation.golden_set import (
    GoldenQuery,
    GoldenTestSet,
    run_golden_tests,
)


class TestGoldenTestSet:
    """Tests for GoldenTestSet class."""

    @pytest.fixture
    def sample_queries_file(self) -> Path:
        """Create temporary golden queries file."""
        queries = {
            "queries": [
                {
                    "query": "What is verification?",
                    "expected_in_top_k": ["verification", "testing"],
                    "category": "verification",
                    "difficulty": "easy",
                },
                {
                    "query": "Requirements documentation",
                    "expected_in_top_k": ["requirements", "shall"],
                    "category": "requirements",
                    "difficulty": "medium",
                },
            ]
        }

        with tempfile.NamedTemporaryFile(
            mode="w", suffix=".yml", delete=False
        ) as f:
            yaml.dump(queries, f)
            return Path(f.name)

    def test_load_queries(self, sample_queries_file: Path) -> None:
        """Test loading queries from YAML."""
        golden = GoldenTestSet(sample_queries_file)
        queries = golden.load_queries()

        assert len(queries) == 2
        assert isinstance(queries[0], GoldenQuery)
        assert queries[0].query == "What is verification?"

    def test_evaluate_single_pass(self, sample_queries_file: Path) -> None:
        """Test evaluation when expected content found."""
        golden = GoldenTestSet(sample_queries_file)
        query = golden.load_queries()[0]

        # Retrieved content contains expected terms
        retrieved = [
            "This discusses verification activities",
            "Testing procedures are important",
        ]

        result = golden.evaluate_single(query, retrieved)

        assert result.passed is True
        assert result.recall == 1.0

    def test_evaluate_single_fail(self, sample_queries_file: Path) -> None:
        """Test evaluation when expected content not found."""
        golden = GoldenTestSet(sample_queries_file)
        query = golden.load_queries()[0]

        # Retrieved content doesn't contain expected terms
        retrieved = [
            "Completely unrelated content",
            "Nothing about the topic",
        ]

        result = golden.evaluate_single(query, retrieved)

        assert result.passed is False
        assert result.recall == 0.0

    def test_evaluate_partial_recall(self, sample_queries_file: Path) -> None:
        """Test evaluation with partial recall."""
        golden = GoldenTestSet(sample_queries_file)
        query = golden.load_queries()[0]

        # Retrieved content contains one of two expected terms
        retrieved = [
            "This discusses verification",
            "But not the other term",
        ]

        result = golden.evaluate_single(query, retrieved)

        assert result.recall == 0.5
        assert result.passed is False  # Below 0.8 threshold

    def test_get_summary(self, sample_queries_file: Path) -> None:
        """Test summary statistics calculation."""
        golden = GoldenTestSet(sample_queries_file)
        queries = golden.load_queries()

        results = [
            golden.evaluate_single(queries[0], ["verification testing"]),
            golden.evaluate_single(queries[1], ["unrelated content"]),
        ]

        summary = golden.get_summary(results)

        assert summary["total"] == 2
        assert summary["passed"] == 1
        assert summary["failed"] == 1
        assert summary["pass_rate"] == 0.5


class TestRunGoldenTests:
    """Tests for run_golden_tests function."""

    @pytest.fixture
    def sample_queries_file(self) -> Path:
        """Create temporary golden queries file."""
        queries = {
            "queries": [
                {
                    "query": "Test query",
                    "expected_in_top_k": ["expected"],
                },
            ]
        }

        with tempfile.NamedTemporaryFile(
            mode="w", suffix=".yml", delete=False
        ) as f:
            yaml.dump(queries, f)
            return Path(f.name)

    def test_run_golden_tests(self, sample_queries_file: Path) -> None:
        """Test running golden tests with mock search function."""
        # Mock search function that returns expected content
        def mock_search(query: str) -> list[dict[str, str]]:
            return [{"content": "This contains expected content"}]

        results, summary = run_golden_tests(
            sample_queries_file,
            mock_search,
            k=5,
        )

        assert len(results) == 1
        assert results[0].passed is True
        assert summary["pass_rate"] == 1.0


# Ensure tests/evaluation/__init__.py exists
```

Create `tests/evaluation/__init__.py`:
```python
"""Evaluation tests package."""
```

Update `src/knowledge_mcp/evaluation/__init__.py`:
```python
"""
Evaluation framework for RAG quality assessment.

This package provides:
- Golden test set management
- RAG Triad metrics (context relevance, faithfulness, answer relevance)
- CLI reporting for evaluation results
"""

from __future__ import annotations

from knowledge_mcp.evaluation.golden_set import (
    GoldenQuery,
    GoldenTestResult,
    GoldenTestSet,
    run_golden_tests,
)
from knowledge_mcp.evaluation.metrics import evaluate_retrieval_only
from knowledge_mcp.evaluation.reporter import (
    print_evaluation_summary,
    print_golden_results,
)

__all__ = [
    "GoldenQuery",
    "GoldenTestResult",
    "GoldenTestSet",
    "run_golden_tests",
    "evaluate_retrieval_only",
    "print_evaluation_summary",
    "print_golden_results",
]
```

Run tests:
```bash
cd mcps/knowledge-mcp
mkdir -p tests/evaluation
poetry run pytest tests/evaluation/ -v
```
  </action>
  <verify>`poetry run pytest tests/evaluation/ -v` shows all tests passing</verify>
  <done>Reporter and 6+ evaluation tests implemented</done>
</task>

</tasks>

<verification>
Run verification sequence:
```bash
cd mcps/knowledge-mcp

# 1. Import test
poetry run python -c "
from knowledge_mcp.evaluation import (
    GoldenTestSet,
    run_golden_tests,
    evaluate_retrieval_only,
    print_evaluation_summary,
    print_golden_results,
)
print('Imports: OK')
"

# 2. Check golden queries file exists
ls -la data/golden_queries.yml
wc -l data/golden_queries.yml

# 3. Load golden queries
poetry run python -c "
from pathlib import Path
from knowledge_mcp.evaluation import GoldenTestSet

g = GoldenTestSet(Path('data/golden_queries.yml'))
queries = g.load_queries()
print(f'Loaded {len(queries)} golden queries')
categories = set(q.category for q in queries)
print(f'Categories: {categories}')
"

# 4. Run evaluation tests
poetry run pytest tests/evaluation/ -v

# 5. Run all tests
poetry run pytest tests/ -v --tb=short
```
</verification>

<success_criteria>
- [ ] metrics.py with evaluate_retrieval_only and evaluate_rag_triad
- [ ] golden_set.py with GoldenTestSet class and run_golden_tests function
- [ ] reporter.py with print_evaluation_summary and print_golden_results
- [ ] data/golden_queries.yml with 30+ queries across categories
- [ ] evaluation __init__.py exports all public APIs
- [ ] 6+ evaluation tests passing
- [ ] No regressions in existing tests
</success_criteria>

<output>
After completion, create `.planning/phases/04-production-readiness/04-04-SUMMARY.md`
</output>
