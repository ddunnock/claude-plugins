---
phase: 04-test-coverage
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - tests/unit/test_utils/test_logging.py
  - tests/unit/test_main.py
autonomous: true
user_setup: []

must_haves:
  truths:
    - "SensitiveDataFilter redaction tested for all patterns"
    - "JSON and Human formatters tested"
    - "setup_logging and get_logger tested"
    - "__main__.py entry point tested"
  artifacts:
    - path: "tests/unit/test_utils/test_logging.py"
      provides: "Unit tests for logging utilities"
      min_lines: 120
    - path: "tests/unit/test_main.py"
      provides: "Unit tests for CLI entry point"
      min_lines: 40
  key_links:
    - from: "tests/unit/test_utils/test_logging.py"
      to: "src/knowledge_mcp/utils/logging.py"
      via: "imports SensitiveDataFilter, setup_logging, get_logger"
      pattern: "from knowledge_mcp.utils.logging import"
    - from: "tests/unit/test_main.py"
      to: "src/knowledge_mcp/__main__.py"
      via: "imports cli function"
      pattern: "from knowledge_mcp.__main__ import cli"
---

<objective>
Add unit tests for logging utilities and CLI entry point.

Purpose: logging.py is at 20% coverage, __main__.py is at 0%. These are important for security (sensitive data filtering) and usability (CLI works correctly).

Output: Two test files covering logging utilities and CLI entry point.
</objective>

<execution_context>
@/Users/dunnock/.claude/get-shit-done/workflows/execute-plan.md
@/Users/dunnock/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

Key files:
@src/knowledge_mcp/utils/logging.py
@src/knowledge_mcp/__main__.py
@tests/conftest.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Logging utilities unit tests</name>
  <files>tests/unit/test_utils/test_logging.py</files>
  <action>
Create comprehensive unit tests for logging.py:

**Test Classes:**

`TestSensitiveDataFilter`:
- `test_redacts_openai_api_key` - verify sk-xxx patterns redacted
- `test_redacts_long_tokens` - verify 32+ char tokens redacted
- `test_redacts_key_value_patterns` - verify "api_key=xxx" redacted
- `test_redacts_sensitive_dict_keys` - verify args dict values redacted for sensitive keys
- `test_preserves_normal_messages` - verify non-sensitive text unchanged
- `test_redacts_tuple_args` - verify string args in tuples redacted
- `test_always_returns_true` - verify filter never drops records

`TestJSONFormatter`:
- `test_formats_as_json` - verify output is valid JSON
- `test_includes_timestamp` - verify timestamp field present
- `test_includes_level` - verify level field present
- `test_includes_message` - verify message field present
- `test_includes_exception_info` - verify exception field when exc_info set
- `test_includes_extra_fields` - verify custom extra fields included
- `test_excludes_standard_attrs` - verify standard LogRecord attrs not duplicated

`TestHumanFormatter`:
- `test_formats_readable_output` - verify human-readable format
- `test_includes_color_codes` - verify ANSI codes for levels
- `test_includes_location` - verify module:func:line format
- `test_includes_exception_traceback` - verify exception formatted

`TestSetupLogging`:
- `test_uses_env_var_level` - verify LOG_LEVEL env var respected
- `test_uses_explicit_level` - verify level parameter works
- `test_uses_json_formatter_when_requested` - verify json_format=True
- `test_uses_human_formatter_by_default` - verify json_format=False
- `test_adds_sensitive_filter` - verify SensitiveDataFilter added
- `test_clears_existing_handlers` - verify no duplicate handlers

`TestGetLogger`:
- `test_returns_child_logger` - verify logger under knowledge_mcp namespace
- `test_prefixes_non_namespaced_names` - verify prefix added if missing

**Key test pattern for SensitiveDataFilter:**
```python
import logging

def test_redacts_openai_api_key():
    filter = SensitiveDataFilter()
    record = logging.LogRecord(
        name="test", level=logging.INFO, pathname="", lineno=0,
        msg="Key is sk-abc123def456xyz789012345678901234567890",
        args=(), exc_info=None
    )
    filter.filter(record)
    assert "sk-abc" not in record.msg
    assert "REDACTED" in record.msg
```
  </action>
  <verify>
```bash
poetry run pytest tests/unit/test_utils/test_logging.py -v
poetry run pytest --cov=src/knowledge_mcp/utils/logging --cov-report=term-missing tests/unit/test_utils/test_logging.py
```
Coverage for logging.py should be >80%.
  </verify>
  <done>logging.py coverage reaches 80%+ with all filter patterns and formatters tested.</done>
</task>

<task type="auto">
  <name>Task 2: CLI entry point tests</name>
  <files>tests/unit/test_main.py</files>
  <action>
Create unit tests for __main__.py:

**Test Classes:**

`TestCli`:
- `test_cli_returns_zero_on_success` - verify success exit code
- `test_cli_returns_130_on_keyboard_interrupt` - verify Ctrl+C handling
- `test_cli_returns_1_on_exception` - verify error exit code
- `test_cli_writes_to_stderr_on_interrupt` - verify message written
- `test_cli_writes_to_stderr_on_error` - verify error message written

**Pattern:** Mock asyncio.run and server_main to control execution:
```python
from unittest.mock import patch, MagicMock

def test_cli_returns_zero_on_success():
    with patch("knowledge_mcp.__main__.asyncio.run") as mock_run:
        mock_run.return_value = None
        with patch("knowledge_mcp.__main__.server_main"):
            from knowledge_mcp.__main__ import cli
            result = cli()
            assert result == 0

def test_cli_returns_130_on_keyboard_interrupt():
    with patch("knowledge_mcp.__main__.asyncio.run") as mock_run:
        mock_run.side_effect = KeyboardInterrupt()
        from knowledge_mcp.__main__ import cli
        result = cli()
        assert result == 130
```

Note: Need to reload module or use importlib to test different scenarios since cli() imports at module level.
  </action>
  <verify>
```bash
poetry run pytest tests/unit/test_main.py -v
poetry run pytest --cov=src/knowledge_mcp/__main__ --cov-report=term-missing tests/unit/test_main.py
```
Coverage for __main__.py should be >80%.
  </verify>
  <done>__main__.py coverage reaches 80%+ with all exit paths tested.</done>
</task>

</tasks>

<verification>
```bash
# Run all new tests
poetry run pytest tests/unit/test_utils/test_logging.py tests/unit/test_main.py -v

# Check combined coverage
poetry run pytest --cov=src/knowledge_mcp/utils/logging --cov=src/knowledge_mcp/__main__ --cov-report=term-missing tests/unit/test_utils/test_logging.py tests/unit/test_main.py
```

Both modules should now have >80% coverage.
</verification>

<success_criteria>
- tests/unit/test_utils/test_logging.py exists with 15+ tests
- tests/unit/test_main.py exists with 5+ tests
- logging.py coverage: 20% -> 80%+
- __main__.py coverage: 0% -> 80%+
- All new tests pass
- Zero pyright errors in test files
</success_criteria>

<output>
After completion, create `.planning/phases/04-test-coverage/04-02-SUMMARY.md`
</output>
