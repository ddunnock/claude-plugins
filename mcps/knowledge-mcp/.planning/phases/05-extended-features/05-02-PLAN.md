---
phase: 05-extended-features
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - src/knowledge_mcp/embed/local_embedder.py
  - src/knowledge_mcp/embed/__init__.py
  - src/knowledge_mcp/utils/config.py
  - pyproject.toml
  - tests/unit/test_embed/test_local_embedder.py
autonomous: true

must_haves:
  truths:
    - "LocalEmbedder generates embeddings without OpenAI API key"
    - "LocalEmbedder works with sentence-transformers models"
    - "Async embed methods don't block the event loop"
    - "Configuration supports EMBEDDING_PROVIDER=local"
  artifacts:
    - path: "src/knowledge_mcp/embed/local_embedder.py"
      provides: "LocalEmbedder implementing BaseEmbedder"
      exports: ["LocalEmbedder"]
      min_lines: 80
    - path: "tests/unit/test_embed/test_local_embedder.py"
      provides: "Unit tests for LocalEmbedder"
      min_lines: 60
  key_links:
    - from: "src/knowledge_mcp/embed/local_embedder.py"
      to: "src/knowledge_mcp/embed/base.py"
      via: "class inheritance"
      pattern: "class LocalEmbedder\\(BaseEmbedder\\)"
    - from: "src/knowledge_mcp/embed/local_embedder.py"
      to: "sentence_transformers"
      via: "SentenceTransformer import"
      pattern: "from sentence_transformers import SentenceTransformer"
    - from: "src/knowledge_mcp/embed/local_embedder.py"
      to: "asyncio"
      via: "run_in_executor for non-blocking"
      pattern: "run_in_executor"
---

<objective>
Implement LocalEmbedder for offline/cost-free embedding generation using sentence-transformers.

Purpose: Enable embedding generation without OpenAI API key, supporting offline operation and reducing costs.
Output: Working LocalEmbedder that generates embeddings using local sentence-transformers models.
</objective>

<execution_context>
@/Users/dunnock/.claude/get-shit-done/workflows/execute-plan.md
@/Users/dunnock/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-extended-features/05-CONTEXT.md
@.planning/phases/05-extended-features/05-RESEARCH.md
@src/knowledge_mcp/embed/base.py
@src/knowledge_mcp/embed/openai_embedder.py
@src/knowledge_mcp/embed/__init__.py
@pyproject.toml
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create LocalEmbedder class</name>
  <files>
    src/knowledge_mcp/embed/local_embedder.py
    src/knowledge_mcp/embed/__init__.py
  </files>
  <action>
1. Create src/knowledge_mcp/embed/local_embedder.py:
   ```python
   """Local embedding using sentence-transformers.

   Provides cost-free, offline-capable embedding generation using
   HuggingFace sentence-transformers models.

   Supported models:
   - all-MiniLM-L6-v2: 384 dimensions, fast (default)
   - all-mpnet-base-v2: 768 dimensions, higher quality

   Example:
       >>> embedder = LocalEmbedder(model_name="all-MiniLM-L6-v2")
       >>> vector = await embedder.embed("What is systems engineering?")
       >>> len(vector)
       384
   """
   from __future__ import annotations

   import asyncio
   from concurrent.futures import ThreadPoolExecutor
   from typing import TYPE_CHECKING

   from knowledge_mcp.embed.base import BaseEmbedder

   if TYPE_CHECKING:
       from collections.abc import Sequence

   class LocalEmbedder(BaseEmbedder):
       """Local embedding using sentence-transformers models."""

       def __init__(
           self,
           model_name: str = "all-MiniLM-L6-v2",
           device: str | None = None,
           normalize_embeddings: bool = True,
       ) -> None:
           """Initialize local embedder.

           Args:
               model_name: HuggingFace model name.
               device: "cuda", "cpu", or None (auto-detect).
               normalize_embeddings: L2-normalize embeddings for cosine similarity.
           """
           # Import inside __init__ for lazy loading
           from sentence_transformers import SentenceTransformer

           self._model_name = model_name
           self._normalize = normalize_embeddings
           self._executor = ThreadPoolExecutor(max_workers=1)

           # Load model (blocking on first call)
           self._model = SentenceTransformer(model_name, device=device)
           self._dimensions: int = self._model.get_sentence_embedding_dimension()

       @property
       def dimensions(self) -> int:
           return self._dimensions

       @property
       def model_name(self) -> str:
           return self._model_name

       async def embed(self, text: str) -> list[float]:
           """Generate embedding asynchronously."""
           loop = asyncio.get_running_loop()
           embedding = await loop.run_in_executor(
               self._executor,
               self._sync_embed,
               text,
           )
           return embedding

       def _sync_embed(self, text: str) -> list[float]:
           """Synchronous embedding (run in executor)."""
           embedding = self._model.encode(
               text,
               normalize_embeddings=self._normalize,
               show_progress_bar=False,
           )
           return embedding.tolist()

       async def embed_batch(
           self,
           texts: Sequence[str],
           *,
           batch_size: int = 32,
       ) -> list[list[float]]:
           """Generate embeddings for batch."""
           loop = asyncio.get_running_loop()
           embeddings = await loop.run_in_executor(
               self._executor,
               self._sync_embed_batch,
               list(texts),
               batch_size,
           )
           return embeddings

       def _sync_embed_batch(
           self,
           texts: list[str],
           batch_size: int,
       ) -> list[list[float]]:
           """Synchronous batch embedding (run in executor)."""
           embeddings = self._model.encode(
               texts,
               batch_size=batch_size,
               normalize_embeddings=self._normalize,
               show_progress_bar=False,
           )
           return [emb.tolist() for emb in embeddings]
   ```

2. Critical implementation details from RESEARCH.md:
   - MUST use `normalize_embeddings=True` for correct cosine similarity
   - MUST wrap sync model.encode() with run_in_executor to avoid blocking event loop
   - Use ThreadPoolExecutor(max_workers=1) to avoid model contention
   - Import SentenceTransformer inside __init__ for lazy loading

3. Update src/knowledge_mcp/embed/__init__.py:
   - Add conditional import of LocalEmbedder
   - Use try/except to handle missing sentence-transformers (optional dependency)
   - Update __all__ to include LocalEmbedder when available
  </action>
  <verify>
    `poetry install --with local && poetry run python -c "from knowledge_mcp.embed import LocalEmbedder; print('OK')"`
  </verify>
  <done>
    - LocalEmbedder implements BaseEmbedder interface
    - Uses run_in_executor for non-blocking async
    - normalize_embeddings=True for correct similarity
  </done>
</task>

<task type="auto">
  <name>Task 2: Update configuration for local embeddings</name>
  <files>
    src/knowledge_mcp/utils/config.py
    pyproject.toml
  </files>
  <action>
1. Read current src/knowledge_mcp/utils/config.py and add:
   - `embedding_provider` field: "openai" (default) or "local"
   - `local_embedding_model` field: "all-MiniLM-L6-v2" (default)
   - Update validation to not require OPENAI_API_KEY when embedding_provider="local"

2. Configuration via environment variables:
   ```
   EMBEDDING_PROVIDER=local           # Use local embeddings
   LOCAL_EMBEDDING_MODEL=all-MiniLM-L6-v2  # Model name
   ```

3. Update pyproject.toml:
   - Update sentence-transformers version: `sentence-transformers = ">=3.0.0"` (in local group)
   - This ensures we get the latest sentence-transformers with better performance

4. Add factory function or method to create appropriate embedder based on config
  </action>
  <verify>
    `poetry install --with local` succeeds
  </verify>
  <done>
    - Config supports EMBEDDING_PROVIDER=local
    - OPENAI_API_KEY not required when using local embeddings
    - sentence-transformers updated to >=3.0.0
  </done>
</task>

<task type="auto">
  <name>Task 3: Add LocalEmbedder tests</name>
  <files>
    tests/unit/test_embed/test_local_embedder.py
  </files>
  <action>
1. Create tests/unit/test_embed/test_local_embedder.py:
   ```python
   """Unit tests for LocalEmbedder."""
   from unittest.mock import MagicMock, patch
   import pytest
   import numpy as np

   class TestLocalEmbedder:
       """Tests for LocalEmbedder class."""

       @pytest.fixture
       def mock_model(self) -> MagicMock:
           """Create mock SentenceTransformer."""
           model = MagicMock()
           model.get_sentence_embedding_dimension.return_value = 384
           model.encode.return_value = np.array([0.1] * 384)
           return model

       @patch("knowledge_mcp.embed.local_embedder.SentenceTransformer")
       def test_dimensions(self, mock_st_cls: MagicMock, mock_model: MagicMock) -> None:
           """Test dimensions property returns model dimensions."""
           mock_st_cls.return_value = mock_model
           from knowledge_mcp.embed.local_embedder import LocalEmbedder

           embedder = LocalEmbedder()
           assert embedder.dimensions == 384

       @patch("knowledge_mcp.embed.local_embedder.SentenceTransformer")
       def test_model_name(self, mock_st_cls: MagicMock, mock_model: MagicMock) -> None:
           """Test model_name returns configured model."""
           mock_st_cls.return_value = mock_model
           from knowledge_mcp.embed.local_embedder import LocalEmbedder

           embedder = LocalEmbedder(model_name="all-mpnet-base-v2")
           assert embedder.model_name == "all-mpnet-base-v2"

       @patch("knowledge_mcp.embed.local_embedder.SentenceTransformer")
       @pytest.mark.asyncio
       async def test_embed(self, mock_st_cls: MagicMock, mock_model: MagicMock) -> None:
           """Test embed returns list of floats."""
           mock_st_cls.return_value = mock_model
           mock_model.encode.return_value = np.array([0.5] * 384)
           from knowledge_mcp.embed.local_embedder import LocalEmbedder

           embedder = LocalEmbedder()
           result = await embedder.embed("test text")

           assert isinstance(result, list)
           assert len(result) == 384
           assert all(isinstance(v, float) for v in result)
           mock_model.encode.assert_called_once()

       @patch("knowledge_mcp.embed.local_embedder.SentenceTransformer")
       @pytest.mark.asyncio
       async def test_embed_batch(self, mock_st_cls: MagicMock, mock_model: MagicMock) -> None:
           """Test embed_batch returns list of embedding lists."""
           mock_st_cls.return_value = mock_model
           mock_model.encode.return_value = np.array([[0.5] * 384, [0.6] * 384])
           from knowledge_mcp.embed.local_embedder import LocalEmbedder

           embedder = LocalEmbedder()
           texts = ["text 1", "text 2"]
           results = await embedder.embed_batch(texts)

           assert len(results) == 2
           assert all(len(r) == 384 for r in results)

       @patch("knowledge_mcp.embed.local_embedder.SentenceTransformer")
       def test_normalize_embeddings_default_true(
           self, mock_st_cls: MagicMock, mock_model: MagicMock
       ) -> None:
           """Test normalize_embeddings defaults to True."""
           mock_st_cls.return_value = mock_model
           from knowledge_mcp.embed.local_embedder import LocalEmbedder

           embedder = LocalEmbedder()
           # Access private attribute to verify
           assert embedder._normalize is True
   ```

2. Test cases to cover:
   - dimensions property returns correct value
   - model_name property returns configured model
   - embed() returns list of floats with correct dimensions
   - embed_batch() returns list of lists
   - normalize_embeddings defaults to True (critical per RESEARCH.md)
   - Custom model name is passed to SentenceTransformer
  </action>
  <verify>
    `poetry run pytest tests/unit/test_embed/test_local_embedder.py -v` passes
  </verify>
  <done>
    - All LocalEmbedder tests pass
    - Tests verify dimensions, model_name, embed, embed_batch
    - Tests confirm normalize_embeddings=True default
  </done>
</task>

</tasks>

<verification>
1. `poetry install --with local` completes successfully
2. `from knowledge_mcp.embed import LocalEmbedder` works
3. `poetry run pytest tests/unit/test_embed/test_local_embedder.py -v` passes
4. `poetry run pyright src/knowledge_mcp/embed/local_embedder.py` reports no errors
5. LocalEmbedder.dimensions returns 384 (for all-MiniLM-L6-v2)
</verification>

<success_criteria>
- LocalEmbedder generates embeddings using sentence-transformers
- Async methods don't block event loop (uses run_in_executor)
- normalize_embeddings=True by default
- Configuration supports EMBEDDING_PROVIDER=local
- All tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/05-extended-features/05-02-SUMMARY.md`
</output>
