---
phase: 05-extended-features
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - src/knowledge_mcp/embed/local_embedder.py
  - src/knowledge_mcp/embed/__init__.py
  - src/knowledge_mcp/utils/config.py
  - pyproject.toml
  - tests/unit/test_embed/test_local_embedder.py
autonomous: true

must_haves:
  truths:
    - "LocalEmbedder generates embeddings without OpenAI API key"
    - "LocalEmbedder works with sentence-transformers models"
    - "Async embed methods don't block the event loop"
    - "Configuration supports EMBEDDING_PROVIDER=local"
    - "create_embedder(config) returns appropriate embedder based on config"
  artifacts:
    - path: "src/knowledge_mcp/embed/local_embedder.py"
      provides: "LocalEmbedder implementing BaseEmbedder"
      exports: ["LocalEmbedder"]
      min_lines: 80
    - path: "src/knowledge_mcp/embed/__init__.py"
      provides: "create_embedder factory function"
      exports: ["create_embedder", "LocalEmbedder", "OpenAIEmbedder"]
    - path: "tests/unit/test_embed/test_local_embedder.py"
      provides: "Unit tests for LocalEmbedder"
      min_lines: 60
  key_links:
    - from: "src/knowledge_mcp/embed/local_embedder.py"
      to: "src/knowledge_mcp/embed/base.py"
      via: "class inheritance"
      pattern: "class LocalEmbedder\\(BaseEmbedder\\)"
    - from: "src/knowledge_mcp/embed/local_embedder.py"
      to: "sentence_transformers"
      via: "SentenceTransformer import"
      pattern: "from sentence_transformers import SentenceTransformer"
    - from: "src/knowledge_mcp/embed/local_embedder.py"
      to: "asyncio"
      via: "run_in_executor for non-blocking"
      pattern: "run_in_executor"
    - from: "src/knowledge_mcp/embed/__init__.py"
      to: "src/knowledge_mcp/utils/config.py"
      via: "create_embedder uses config.embedding_provider"
      pattern: "def create_embedder.*config"
---

<objective>
Implement LocalEmbedder for offline/cost-free embedding generation using sentence-transformers.

Purpose: Enable embedding generation without OpenAI API key, supporting offline operation and reducing costs.
Output: Working LocalEmbedder that generates embeddings using local sentence-transformers models, plus factory function for embedder selection.
</objective>

<execution_context>
@/Users/dunnock/.claude/get-shit-done/workflows/execute-plan.md
@/Users/dunnock/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-extended-features/05-CONTEXT.md
@.planning/phases/05-extended-features/05-RESEARCH.md
@src/knowledge_mcp/embed/base.py
@src/knowledge_mcp/embed/openai_embedder.py
@src/knowledge_mcp/embed/__init__.py
@pyproject.toml
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create LocalEmbedder class</name>
  <files>
    src/knowledge_mcp/embed/local_embedder.py
  </files>
  <action>
1. Create src/knowledge_mcp/embed/local_embedder.py:
   ```python
   """Local embedding using sentence-transformers.

   Provides cost-free, offline-capable embedding generation using
   HuggingFace sentence-transformers models.

   Supported models:
   - all-MiniLM-L6-v2: 384 dimensions, fast (default)
   - all-mpnet-base-v2: 768 dimensions, higher quality

   Example:
       >>> embedder = LocalEmbedder(model_name="all-MiniLM-L6-v2")
       >>> vector = await embedder.embed("What is systems engineering?")
       >>> len(vector)
       384
   """
   from __future__ import annotations

   import asyncio
   from concurrent.futures import ThreadPoolExecutor
   from typing import TYPE_CHECKING

   from knowledge_mcp.embed.base import BaseEmbedder

   if TYPE_CHECKING:
       from collections.abc import Sequence

   class LocalEmbedder(BaseEmbedder):
       """Local embedding using sentence-transformers models."""

       def __init__(
           self,
           model_name: str = "all-MiniLM-L6-v2",
           device: str | None = None,
           normalize_embeddings: bool = True,
       ) -> None:
           """Initialize local embedder.

           Args:
               model_name: HuggingFace model name.
               device: "cuda", "cpu", or None (auto-detect).
               normalize_embeddings: L2-normalize embeddings for cosine similarity.
           """
           # Import inside __init__ for lazy loading
           from sentence_transformers import SentenceTransformer

           self._model_name = model_name
           self._normalize = normalize_embeddings
           self._executor = ThreadPoolExecutor(max_workers=1)

           # Load model (blocking on first call)
           self._model = SentenceTransformer(model_name, device=device)
           self._dimensions: int = self._model.get_sentence_embedding_dimension()

       @property
       def dimensions(self) -> int:
           return self._dimensions

       @property
       def model_name(self) -> str:
           return self._model_name

       async def embed(self, text: str) -> list[float]:
           """Generate embedding asynchronously."""
           loop = asyncio.get_running_loop()
           embedding = await loop.run_in_executor(
               self._executor,
               self._sync_embed,
               text,
           )
           return embedding

       def _sync_embed(self, text: str) -> list[float]:
           """Synchronous embedding (run in executor)."""
           embedding = self._model.encode(
               text,
               normalize_embeddings=self._normalize,
               show_progress_bar=False,
           )
           return embedding.tolist()

       async def embed_batch(
           self,
           texts: Sequence[str],
           *,
           batch_size: int = 32,
       ) -> list[list[float]]:
           """Generate embeddings for batch."""
           loop = asyncio.get_running_loop()
           embeddings = await loop.run_in_executor(
               self._executor,
               self._sync_embed_batch,
               list(texts),
               batch_size,
           )
           return embeddings

       def _sync_embed_batch(
           self,
           texts: list[str],
           batch_size: int,
       ) -> list[list[float]]:
           """Synchronous batch embedding (run in executor)."""
           embeddings = self._model.encode(
               texts,
               batch_size=batch_size,
               normalize_embeddings=self._normalize,
               show_progress_bar=False,
           )
           return [emb.tolist() for emb in embeddings]
   ```

2. Critical implementation details from RESEARCH.md:
   - MUST use `normalize_embeddings=True` for correct cosine similarity
   - MUST wrap sync model.encode() with run_in_executor to avoid blocking event loop
   - Use ThreadPoolExecutor(max_workers=1) to avoid model contention
   - Import SentenceTransformer inside __init__ for lazy loading
  </action>
  <verify>
    `poetry install --with local && poetry run python -c "from knowledge_mcp.embed.local_embedder import LocalEmbedder; print('OK')"`
  </verify>
  <done>
    - LocalEmbedder implements BaseEmbedder interface
    - Uses run_in_executor for non-blocking async
    - normalize_embeddings=True for correct similarity
  </done>
</task>

<task type="auto">
  <name>Task 2: Update configuration and add factory function</name>
  <files>
    src/knowledge_mcp/utils/config.py
    src/knowledge_mcp/embed/__init__.py
    pyproject.toml
  </files>
  <action>
1. Update src/knowledge_mcp/utils/config.py to add embedding provider config:
   - Add `embedding_provider` field with type `Literal["openai", "local"]`, default "openai"
   - Add `local_embedding_model` field with type `str`, default "all-MiniLM-L6-v2"
   - Update `validate()` method: only require OPENAI_API_KEY when embedding_provider="openai"

   Add these fields to KnowledgeConfig class (after embedding_dimensions):
   ```python
   # Embedding Provider Selection
   embedding_provider: Literal["openai", "local"] = Field(
       default="openai",
       description="Embedding provider: openai or local",
   )
   local_embedding_model: str = Field(
       default="all-MiniLM-L6-v2",
       description="Local embedding model (sentence-transformers)",
   )
   ```

   Update validate() method:
   ```python
   def validate(self) -> list[str]:
       errors: list[str] = []

       # Only require OpenAI API key when using OpenAI embeddings
       if self.embedding_provider == "openai" and not self.openai_api_key:
           errors.append("OPENAI_API_KEY is required when embedding_provider=openai")

       if self.vector_store == "qdrant":
           if not self.qdrant_url:
               errors.append("QDRANT_URL is required when using Qdrant")
           if not self.qdrant_api_key:
               errors.append("QDRANT_API_KEY is required for Qdrant Cloud")

       return errors
   ```

   Update load_config() to read new env vars:
   ```python
   embedding_provider=os.getenv("EMBEDDING_PROVIDER", "openai"),  # type: ignore[arg-type]
   local_embedding_model=os.getenv("LOCAL_EMBEDDING_MODEL", "all-MiniLM-L6-v2"),
   ```

2. Update src/knowledge_mcp/embed/__init__.py to add create_embedder factory:
   ```python
   """Embedding generation module.

   Provides embedders for generating vector embeddings from text.
   Use create_embedder(config) to get the appropriate embedder based on configuration.
   """
   from __future__ import annotations

   from typing import TYPE_CHECKING

   from knowledge_mcp.embed.base import BaseEmbedder
   from knowledge_mcp.embed.openai_embedder import OpenAIEmbedder

   if TYPE_CHECKING:
       from knowledge_mcp.utils.config import KnowledgeConfig

   __all__ = ["BaseEmbedder", "OpenAIEmbedder", "create_embedder"]

   # Conditionally export LocalEmbedder if sentence-transformers is available
   try:
       from knowledge_mcp.embed.local_embedder import LocalEmbedder
       __all__.append("LocalEmbedder")
       _HAS_LOCAL = True
   except ImportError:
       _HAS_LOCAL = False


   def create_embedder(config: KnowledgeConfig) -> BaseEmbedder:
       """Create embedder based on configuration.

       Factory function that returns the appropriate embedder instance
       based on config.embedding_provider setting.

       Args:
           config: KnowledgeConfig instance with embedding settings.

       Returns:
           BaseEmbedder instance (OpenAIEmbedder or LocalEmbedder).

       Raises:
           ValueError: If provider is "local" but sentence-transformers not installed.
           ValueError: If provider is unknown.

       Example:
           >>> config = load_config()
           >>> embedder = create_embedder(config)
           >>> vector = await embedder.embed("test query")
       """
       if config.embedding_provider == "openai":
           return OpenAIEmbedder(
               api_key=config.openai_api_key,
               model=config.embedding_model,
               dimensions=config.embedding_dimensions,
           )
       elif config.embedding_provider == "local":
           if not _HAS_LOCAL:
               raise ValueError(
                   "Local embeddings require sentence-transformers. "
                   "Install with: poetry install --with local"
               )
           return LocalEmbedder(model_name=config.local_embedding_model)
       else:
           raise ValueError(f"Unknown embedding provider: {config.embedding_provider}")
   ```

3. Update pyproject.toml:
   - Ensure sentence-transformers version in local group: `sentence-transformers = ">=3.0.0"`
  </action>
  <verify>
    - `poetry install --with local` succeeds
    - `EMBEDDING_PROVIDER=local poetry run python -c "from knowledge_mcp.embed import create_embedder; from knowledge_mcp.utils.config import load_config; c = load_config(); print(c.embedding_provider)"`
  </verify>
  <done>
    - Config supports EMBEDDING_PROVIDER=local and LOCAL_EMBEDDING_MODEL
    - OPENAI_API_KEY not required when using local embeddings
    - create_embedder(config) factory returns appropriate embedder
    - sentence-transformers updated to >=3.0.0
  </done>
</task>

<task type="auto">
  <name>Task 3: Add LocalEmbedder tests</name>
  <files>
    tests/unit/test_embed/test_local_embedder.py
  </files>
  <action>
1. Create tests/unit/test_embed/test_local_embedder.py:
   ```python
   """Unit tests for LocalEmbedder."""
   from unittest.mock import MagicMock, patch
   import pytest
   import numpy as np

   class TestLocalEmbedder:
       """Tests for LocalEmbedder class."""

       @pytest.fixture
       def mock_model(self) -> MagicMock:
           """Create mock SentenceTransformer."""
           model = MagicMock()
           model.get_sentence_embedding_dimension.return_value = 384
           model.encode.return_value = np.array([0.1] * 384)
           return model

       @patch("knowledge_mcp.embed.local_embedder.SentenceTransformer")
       def test_dimensions(self, mock_st_cls: MagicMock, mock_model: MagicMock) -> None:
           """Test dimensions property returns model dimensions."""
           mock_st_cls.return_value = mock_model
           from knowledge_mcp.embed.local_embedder import LocalEmbedder

           embedder = LocalEmbedder()
           assert embedder.dimensions == 384

       @patch("knowledge_mcp.embed.local_embedder.SentenceTransformer")
       def test_model_name(self, mock_st_cls: MagicMock, mock_model: MagicMock) -> None:
           """Test model_name returns configured model."""
           mock_st_cls.return_value = mock_model
           from knowledge_mcp.embed.local_embedder import LocalEmbedder

           embedder = LocalEmbedder(model_name="all-mpnet-base-v2")
           assert embedder.model_name == "all-mpnet-base-v2"

       @patch("knowledge_mcp.embed.local_embedder.SentenceTransformer")
       @pytest.mark.asyncio
       async def test_embed(self, mock_st_cls: MagicMock, mock_model: MagicMock) -> None:
           """Test embed returns list of floats."""
           mock_st_cls.return_value = mock_model
           mock_model.encode.return_value = np.array([0.5] * 384)
           from knowledge_mcp.embed.local_embedder import LocalEmbedder

           embedder = LocalEmbedder()
           result = await embedder.embed("test text")

           assert isinstance(result, list)
           assert len(result) == 384
           assert all(isinstance(v, float) for v in result)
           mock_model.encode.assert_called_once()

       @patch("knowledge_mcp.embed.local_embedder.SentenceTransformer")
       @pytest.mark.asyncio
       async def test_embed_batch(self, mock_st_cls: MagicMock, mock_model: MagicMock) -> None:
           """Test embed_batch returns list of embedding lists."""
           mock_st_cls.return_value = mock_model
           mock_model.encode.return_value = np.array([[0.5] * 384, [0.6] * 384])
           from knowledge_mcp.embed.local_embedder import LocalEmbedder

           embedder = LocalEmbedder()
           texts = ["text 1", "text 2"]
           results = await embedder.embed_batch(texts)

           assert len(results) == 2
           assert all(len(r) == 384 for r in results)

       @patch("knowledge_mcp.embed.local_embedder.SentenceTransformer")
       def test_normalize_embeddings_default_true(
           self, mock_st_cls: MagicMock, mock_model: MagicMock
       ) -> None:
           """Test normalize_embeddings defaults to True."""
           mock_st_cls.return_value = mock_model
           from knowledge_mcp.embed.local_embedder import LocalEmbedder

           embedder = LocalEmbedder()
           # Access private attribute to verify
           assert embedder._normalize is True
   ```

2. Test cases to cover:
   - dimensions property returns correct value
   - model_name property returns configured model
   - embed() returns list of floats with correct dimensions
   - embed_batch() returns list of lists
   - normalize_embeddings defaults to True (critical per RESEARCH.md)
   - Custom model name is passed to SentenceTransformer
  </action>
  <verify>
    `poetry run pytest tests/unit/test_embed/test_local_embedder.py -v` passes
  </verify>
  <done>
    - All LocalEmbedder tests pass
    - Tests verify dimensions, model_name, embed, embed_batch
    - Tests confirm normalize_embeddings=True default
  </done>
</task>

</tasks>

<verification>
1. `poetry install --with local` completes successfully
2. `from knowledge_mcp.embed import LocalEmbedder, create_embedder` works
3. `poetry run pytest tests/unit/test_embed/test_local_embedder.py -v` passes
4. `poetry run pyright src/knowledge_mcp/embed/local_embedder.py` reports no errors
5. LocalEmbedder.dimensions returns 384 (for all-MiniLM-L6-v2)
6. create_embedder(config) returns LocalEmbedder when config.embedding_provider="local"
</verification>

<success_criteria>
- LocalEmbedder generates embeddings using sentence-transformers
- Async methods don't block event loop (uses run_in_executor)
- normalize_embeddings=True by default
- Configuration supports EMBEDDING_PROVIDER=local
- create_embedder factory function works correctly
- All tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/05-extended-features/05-02-SUMMARY.md`
</output>
