---
phase: 01-core-acquisition
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - src/knowledge_mcp/ingest/__init__.py
  - src/knowledge_mcp/ingest/web_ingestor.py
  - pyproject.toml
autonomous: true

must_haves:
  truths:
    - "Crawl4AI crawls a URL and returns Markdown content"
    - "robots.txt compliance is checked before crawling"
    - "Rate limiting prevents overwhelming target servers"
    - "Failed crawls return structured error information"
  artifacts:
    - path: "src/knowledge_mcp/ingest/web_ingestor.py"
      provides: "Web content ingestion via Crawl4AI"
      exports: ["WebIngestor", "WebIngestionResult"]
      min_lines: 150
  key_links:
    - from: "src/knowledge_mcp/ingest/web_ingestor.py"
      to: "crawl4ai"
      via: "AsyncWebCrawler import"
      pattern: "from crawl4ai import"
    - from: "src/knowledge_mcp/ingest/web_ingestor.py"
      to: "result.markdown"
      via: "Markdown extraction from CrawlResult"
      pattern: "result\\.markdown"
---

<objective>
Implement Crawl4AI-based web content ingestion with robots.txt compliance, rate limiting, and structured error handling.

Purpose: Enable web content acquisition for the knowledge base (FR-2.1: Web content ingestion).
Output: WebIngestor class that crawls URLs and returns clean Markdown suitable for chunking.
</objective>

<execution_context>
@/Users/dunnock/.claude/get-shit-done/workflows/execute-plan.md
@/Users/dunnock/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-core-acquisition/01-RESEARCH.md
@.planning/research/crawl4ai.md

# Existing ingestion patterns
@src/knowledge_mcp/ingest/base.py
@src/knowledge_mcp/ingest/pdf_ingestor.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add Crawl4AI dependency</name>
  <files>pyproject.toml</files>
  <action>
    Add Crawl4AI dependency to pyproject.toml:

    ```toml
    crawl4ai = "^0.7.8"
    ```

    Pin to ^0.7.8 (not 0.8.0) for stability as recommended in research.

    After adding, run:
    ```bash
    poetry install
    crawl4ai-setup  # Install Playwright browsers
    ```

    Note: If crawl4ai-setup fails in CI/testing, that's expected. Browser setup is
    only needed for actual crawling, not for import checks.

    AVOID: Using crawl4ai ^0.8.0 initially.
    WHY: New release may have undiscovered issues; 0.7.8 is proven stable.
  </action>
  <verify>
    poetry install completes.
    python -c "import crawl4ai; print(crawl4ai.__version__)"
  </verify>
  <done>
    crawl4ai ^0.7.8 added to pyproject.toml.
    poetry install succeeds.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create WebIngestor class</name>
  <files>src/knowledge_mcp/ingest/web_ingestor.py, src/knowledge_mcp/ingest/__init__.py</files>
  <action>
    Create src/knowledge_mcp/ingest/web_ingestor.py:

    ```python
    """
    Web content ingestion via Crawl4AI.

    Provides async web crawling with robots.txt compliance, rate limiting,
    and clean Markdown output suitable for chunking and embedding.

    Example:
        >>> ingestor = WebIngestor()
        >>> result = await ingestor.ingest("https://example.com/page")
        >>> if result.success:
        ...     print(result.markdown[:100])
    """

    from __future__ import annotations

    import asyncio
    from dataclasses import dataclass
    from typing import Optional, List

    from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
    from crawl4ai.dispatcher import SemaphoreDispatcher, RateLimiter


    @dataclass
    class WebIngestionResult:
        """Result of a web ingestion attempt."""
        url: str
        success: bool
        markdown: Optional[str] = None
        title: Optional[str] = None
        word_count: int = 0
        error: Optional[str] = None
        status_code: Optional[int] = None
        final_url: Optional[str] = None  # After redirects


    @dataclass
    class WebIngestorConfig:
        """Configuration for web ingestion."""
        max_concurrent: int = 3
        base_delay: tuple[float, float] = (1.0, 2.0)  # Random delay range
        max_delay: float = 30.0
        check_robots_txt: bool = True
        word_count_threshold: int = 50  # Minimum words for valid content
        headless: bool = True
        verbose: bool = False


    class WebIngestor:
        """
        Async web content ingestor using Crawl4AI.

        Features:
        - robots.txt compliance (configurable)
        - Rate limiting via SemaphoreDispatcher
        - Clean Markdown output
        - Structured error handling

        Example:
            >>> config = WebIngestorConfig(max_concurrent=5)
            >>> ingestor = WebIngestor(config)
            >>> result = await ingestor.ingest("https://docs.example.com")
        """

        def __init__(self, config: Optional[WebIngestorConfig] = None):
            """
            Initialize web ingestor.

            Args:
                config: Configuration options. Uses defaults if None.
            """
            self.config = config or WebIngestorConfig()
            self._browser_config = BrowserConfig(
                headless=self.config.headless,
                verbose=self.config.verbose,
                text_mode=False,  # Keep images for reference
            )

        def _create_run_config(self) -> CrawlerRunConfig:
            """Create per-crawl configuration."""
            return CrawlerRunConfig(
                cache_mode=CacheMode.BYPASS,
                check_robots_txt=self.config.check_robots_txt,
                word_count_threshold=self.config.word_count_threshold,
                remove_overlay_elements=True,
                exclude_external_links=False,
                process_iframes=True,
            )

        def _create_dispatcher(self) -> SemaphoreDispatcher:
            """Create rate-limiting dispatcher."""
            return SemaphoreDispatcher(
                semaphore_count=self.config.max_concurrent,
                rate_limiter=RateLimiter(
                    base_delay=self.config.base_delay,
                    max_delay=self.config.max_delay,
                )
            )

        async def ingest(self, url: str) -> WebIngestionResult:
            """
            Ingest a single URL.

            Args:
                url: URL to crawl.

            Returns:
                WebIngestionResult with markdown content or error details.
            """
            run_config = self._create_run_config()

            try:
                async with AsyncWebCrawler(config=self._browser_config) as crawler:
                    result = await crawler.arun(url=url, config=run_config)

                    if not result.success:
                        return WebIngestionResult(
                            url=url,
                            success=False,
                            error=result.error_message or "Unknown crawl error",
                            status_code=result.status_code,
                            final_url=result.url,
                        )

                    # Check for robots.txt block (returns 403)
                    if result.status_code == 403:
                        return WebIngestionResult(
                            url=url,
                            success=False,
                            error="Blocked by robots.txt or access forbidden",
                            status_code=403,
                            final_url=result.url,
                        )

                    # Extract markdown
                    markdown = result.markdown.raw_markdown if result.markdown else ""
                    word_count = len(markdown.split())

                    # Validate content quality
                    if word_count < self.config.word_count_threshold:
                        return WebIngestionResult(
                            url=url,
                            success=False,
                            error=f"Insufficient content: {word_count} words (minimum: {self.config.word_count_threshold})",
                            word_count=word_count,
                            final_url=result.url,
                        )

                    # Extract title from HTML
                    title = self._extract_title(result.html) if result.html else None

                    return WebIngestionResult(
                        url=url,
                        success=True,
                        markdown=markdown,
                        title=title,
                        word_count=word_count,
                        status_code=result.status_code,
                        final_url=result.url,
                    )

            except Exception as e:
                return WebIngestionResult(
                    url=url,
                    success=False,
                    error=f"Crawl exception: {str(e)}",
                )

        async def ingest_many(self, urls: List[str]) -> List[WebIngestionResult]:
            """
            Ingest multiple URLs with rate limiting.

            Args:
                urls: List of URLs to crawl.

            Returns:
                List of WebIngestionResult for each URL.
            """
            run_config = self._create_run_config()
            dispatcher = self._create_dispatcher()

            results: List[WebIngestionResult] = []

            try:
                async with AsyncWebCrawler(config=self._browser_config) as crawler:
                    async for result in await crawler.arun_many(
                        urls=urls,
                        config=run_config,
                        dispatcher=dispatcher,
                    ):
                        if not result.success:
                            results.append(WebIngestionResult(
                                url=result.url,
                                success=False,
                                error=result.error_message,
                                status_code=result.status_code,
                            ))
                            continue

                        markdown = result.markdown.raw_markdown if result.markdown else ""
                        word_count = len(markdown.split())

                        if word_count < self.config.word_count_threshold:
                            results.append(WebIngestionResult(
                                url=result.url,
                                success=False,
                                error=f"Insufficient content: {word_count} words",
                                word_count=word_count,
                            ))
                            continue

                        title = self._extract_title(result.html) if result.html else None

                        results.append(WebIngestionResult(
                            url=result.url,
                            success=True,
                            markdown=markdown,
                            title=title,
                            word_count=word_count,
                            status_code=result.status_code,
                            final_url=result.url,
                        ))

            except Exception as e:
                # If crawler fails completely, mark remaining URLs as failed
                processed_urls = {r.url for r in results}
                for url in urls:
                    if url not in processed_urls:
                        results.append(WebIngestionResult(
                            url=url,
                            success=False,
                            error=f"Batch crawl exception: {str(e)}",
                        ))

            return results

        def _extract_title(self, html: str) -> Optional[str]:
            """Extract page title from HTML."""
            try:
                # Simple regex extraction to avoid BeautifulSoup dependency
                import re
                match = re.search(r'<title[^>]*>([^<]+)</title>', html, re.IGNORECASE)
                if match:
                    title = match.group(1).strip()
                    # Clean up common title suffixes
                    for suffix in [' | ', ' - ', ' :: ', ' // ']:
                        if suffix in title:
                            title = title.split(suffix)[0].strip()
                    return title[:500]  # Limit title length
            except Exception:
                pass
            return None


    async def check_url_accessible(url: str, check_robots: bool = True) -> tuple[bool, Optional[str]]:
        """
        Quick check if URL is accessible (preflight).

        Args:
            url: URL to check.
            check_robots: Whether to check robots.txt compliance.

        Returns:
            Tuple of (is_accessible, error_message).
        """
        config = WebIngestorConfig(
            check_robots_txt=check_robots,
            word_count_threshold=0,  # Don't validate content for preflight
        )
        ingestor = WebIngestor(config)
        result = await ingestor.ingest(url)

        if result.success:
            return True, None
        else:
            return False, result.error
    ```

    Update src/knowledge_mcp/ingest/__init__.py to export:
    ```python
    from .web_ingestor import WebIngestor, WebIngestionResult, WebIngestorConfig, check_url_accessible
    ```

    AVOID: Using BeautifulSoup for simple title extraction.
    WHY: Reduces dependencies; regex sufficient for title tag.
  </action>
  <verify>
    python -c "from knowledge_mcp.ingest import WebIngestor, WebIngestionResult, check_url_accessible"
    pyright src/knowledge_mcp/ingest/web_ingestor.py
    ruff check src/knowledge_mcp/ingest/web_ingestor.py
  </verify>
  <done>
    WebIngestor class with ingest() and ingest_many() methods.
    WebIngestionResult dataclass with all result fields.
    check_url_accessible() utility for preflight checks.
    Rate limiting via SemaphoreDispatcher.
    robots.txt compliance enabled by default.
  </done>
</task>

<task type="auto">
  <name>Task 3: Add unit tests for WebIngestor</name>
  <files>tests/unit/test_ingest/test_web_ingestor.py</files>
  <action>
    Create tests/unit/test_ingest/test_web_ingestor.py:

    ```python
    """Unit tests for WebIngestor."""

    from unittest.mock import AsyncMock, MagicMock, patch
    import pytest

    from knowledge_mcp.ingest.web_ingestor import (
        WebIngestor,
        WebIngestionResult,
        WebIngestorConfig,
        check_url_accessible,
    )


    class TestWebIngestorConfig:
        """Tests for WebIngestorConfig."""

        def test_default_values(self) -> None:
            """Test default configuration values."""
            config = WebIngestorConfig()
            assert config.max_concurrent == 3
            assert config.check_robots_txt is True
            assert config.word_count_threshold == 50

        def test_custom_values(self) -> None:
            """Test custom configuration values."""
            config = WebIngestorConfig(
                max_concurrent=5,
                check_robots_txt=False,
                word_count_threshold=100,
            )
            assert config.max_concurrent == 5
            assert config.check_robots_txt is False
            assert config.word_count_threshold == 100


    class TestWebIngestionResult:
        """Tests for WebIngestionResult."""

        def test_success_result(self) -> None:
            """Test successful result structure."""
            result = WebIngestionResult(
                url="https://example.com",
                success=True,
                markdown="# Hello World",
                title="Example",
                word_count=2,
            )
            assert result.success is True
            assert result.markdown == "# Hello World"
            assert result.error is None

        def test_failure_result(self) -> None:
            """Test failure result structure."""
            result = WebIngestionResult(
                url="https://example.com",
                success=False,
                error="Connection timeout",
                status_code=None,
            )
            assert result.success is False
            assert result.error == "Connection timeout"
            assert result.markdown is None


    class TestWebIngestor:
        """Tests for WebIngestor class."""

        @pytest.fixture
        def ingestor(self) -> WebIngestor:
            """Create ingestor with test config."""
            config = WebIngestorConfig(
                check_robots_txt=True,
                word_count_threshold=10,
            )
            return WebIngestor(config)

        def test_init_with_default_config(self) -> None:
            """Test initialization with default config."""
            ingestor = WebIngestor()
            assert ingestor.config.max_concurrent == 3

        def test_init_with_custom_config(self) -> None:
            """Test initialization with custom config."""
            config = WebIngestorConfig(max_concurrent=10)
            ingestor = WebIngestor(config)
            assert ingestor.config.max_concurrent == 10

        def test_create_run_config(self, ingestor: WebIngestor) -> None:
            """Test run config creation."""
            run_config = ingestor._create_run_config()
            # Verify key settings from research
            assert run_config.check_robots_txt is True

        def test_extract_title_simple(self, ingestor: WebIngestor) -> None:
            """Test title extraction from simple HTML."""
            html = "<html><head><title>Test Page</title></head></html>"
            title = ingestor._extract_title(html)
            assert title == "Test Page"

        def test_extract_title_with_suffix(self, ingestor: WebIngestor) -> None:
            """Test title extraction removes common suffixes."""
            html = "<html><head><title>Test Page | Company Name</title></head></html>"
            title = ingestor._extract_title(html)
            assert title == "Test Page"

        def test_extract_title_empty(self, ingestor: WebIngestor) -> None:
            """Test title extraction from HTML without title."""
            html = "<html><body>No title here</body></html>"
            title = ingestor._extract_title(html)
            assert title is None

        @pytest.mark.asyncio
        async def test_ingest_handles_exception(self, ingestor: WebIngestor) -> None:
            """Test ingest handles crawler exceptions gracefully."""
            # Mock AsyncWebCrawler to raise exception
            with patch('knowledge_mcp.ingest.web_ingestor.AsyncWebCrawler') as mock_crawler:
                mock_crawler.return_value.__aenter__ = AsyncMock(side_effect=RuntimeError("Browser crash"))

                result = await ingestor.ingest("https://example.com")

                assert result.success is False
                assert "Crawl exception" in result.error


    class TestCheckUrlAccessible:
        """Tests for check_url_accessible function."""

        @pytest.mark.asyncio
        async def test_returns_tuple(self) -> None:
            """Test function returns correct tuple structure."""
            # Mock the ingestor
            with patch('knowledge_mcp.ingest.web_ingestor.WebIngestor') as MockIngestor:
                mock_instance = MockIngestor.return_value
                mock_instance.ingest = AsyncMock(return_value=WebIngestionResult(
                    url="https://example.com",
                    success=True,
                    markdown="content",
                ))

                accessible, error = await check_url_accessible("https://example.com")

                assert accessible is True
                assert error is None

        @pytest.mark.asyncio
        async def test_returns_error_on_failure(self) -> None:
            """Test function returns error message on failure."""
            with patch('knowledge_mcp.ingest.web_ingestor.WebIngestor') as MockIngestor:
                mock_instance = MockIngestor.return_value
                mock_instance.ingest = AsyncMock(return_value=WebIngestionResult(
                    url="https://example.com",
                    success=False,
                    error="Blocked by robots.txt",
                ))

                accessible, error = await check_url_accessible("https://example.com")

                assert accessible is False
                assert error == "Blocked by robots.txt"
    ```

    Ensure tests/unit/test_ingest/__init__.py exists.

    AVOID: Testing actual network calls in unit tests.
    WHY: Unit tests should be fast and deterministic; use mocks for external services.
  </action>
  <verify>
    pytest tests/unit/test_ingest/test_web_ingestor.py -v
  </verify>
  <done>
    Unit tests for WebIngestorConfig, WebIngestionResult, WebIngestor.
    Tests cover config defaults, title extraction, error handling.
    All tests pass without network calls (mocked).
  </done>
</task>

</tasks>

<verification>
After all tasks complete:

1. **Dependency installed:**
   ```bash
   python -c "import crawl4ai; print(f'Crawl4AI {crawl4ai.__version__}')"
   ```

2. **Imports work:**
   ```bash
   python -c "from knowledge_mcp.ingest import WebIngestor, WebIngestionResult, check_url_accessible"
   ```

3. **Type checking passes:**
   ```bash
   pyright src/knowledge_mcp/ingest/web_ingestor.py
   ```

4. **Linting passes:**
   ```bash
   ruff check src/knowledge_mcp/ingest/
   ```

5. **Unit tests pass:**
   ```bash
   pytest tests/unit/test_ingest/test_web_ingestor.py -v
   ```
</verification>

<success_criteria>
- [ ] crawl4ai ^0.7.8 added to pyproject.toml
- [ ] WebIngestor class with ingest() and ingest_many() methods
- [ ] WebIngestionResult dataclass with url, success, markdown, title, word_count, error, status_code
- [ ] WebIngestorConfig with configurable options
- [ ] check_url_accessible() utility function
- [ ] Rate limiting via SemaphoreDispatcher (default 3 concurrent, 1-2s delay)
- [ ] robots.txt compliance enabled by default
- [ ] Unit tests pass
- [ ] All code passes pyright and ruff checks
</success_criteria>

<output>
After completion, create `.planning/phases/01-core-acquisition/01-02-SUMMARY.md`
</output>
