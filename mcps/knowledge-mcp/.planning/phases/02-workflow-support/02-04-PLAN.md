---
phase: 02-workflow-support
plan: 04
type: execute
wave: 3
depends_on: ["02-02"]
files_modified:
  - src/knowledge_mcp/search/strategies/trade.py
  - tests/unit/test_search/test_trade_strategy.py
autonomous: true

must_haves:
  truths:
    - "TradeStudyStrategy groups results by alternative"
    - "TradeStudyStrategy synthesizes criteria evidence from any content"
    - "TradeStudyStrategy boosts results with quantitative data"
    - "Format output organizes by alternative with criteria per option"
  artifacts:
    - path: "src/knowledge_mcp/search/strategies/trade.py"
      provides: "TradeStudyStrategy class"
      contains: "class TradeStudyStrategy"
    - path: "tests/unit/test_search/test_trade_strategy.py"
      provides: "Unit tests for trade study strategy"
      contains: "class TestTradeStudyStrategy"
  key_links:
    - from: "TradeStudyStrategy"
      to: "SearchStrategy"
      via: "inheritance"
      pattern: "class TradeStudyStrategy.*SearchStrategy"
---

<objective>
Implement TradeStudyStrategy for trade study decision support searches.

Purpose: Enable alternative-grouped results with synthesized criteria evidence, allowing trade studies even when formal comparison documents don't exist in the corpus.

Output: TradeStudyStrategy class that groups results by alternative and extracts criteria evidence.
</objective>

<execution_context>
@/Users/dunnock/.claude/get-shit-done/workflows/execute-plan.md
@/Users/dunnock/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/02-workflow-support/02-RESEARCH.md
@.planning/phases/02-workflow-support/02-CONTEXT.md

# Strategy base (from 02-02)
@src/knowledge_mcp/search/strategies/base.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create TradeStudyStrategy implementation</name>
  <files>src/knowledge_mcp/search/strategies/trade.py</files>
  <action>
Create the Trade Study strategy following the research document patterns:

```python
"""Trade Study search strategy.

Provides alternative-grouped search results with synthesized criteria
evidence for decision support workflows.

Example:
    >>> from knowledge_mcp.search.strategies.trade import TradeStudyStrategy
    >>> strategy = TradeStudyStrategy()
    >>> output = strategy.format_output(results, {
    ...     "alternatives": ["PostgreSQL", "MongoDB"],
    ...     "criteria": ["performance", "cost"]
    ... })
"""

from __future__ import annotations

import re
from collections import defaultdict
from dataclasses import dataclass, field
from typing import Any

from knowledge_mcp.search.models import SearchResult
from knowledge_mcp.search.strategies.base import SearchQuery, SearchStrategy


@dataclass
class CriteriaEvidence:
    """Evidence for a single evaluation criterion.

    Attributes:
        name: Criterion name (e.g., "cost", "performance").
        evidence: Extracted text supporting the criterion.
        value: Quantitative value if found (e.g., "$500", "100ms").
        score: Relevance score from search.
        source: Document source of the evidence.
    """

    name: str
    evidence: str
    value: str | None = None
    score: float = 0.0
    source: str = ""


@dataclass
class AlternativeEvaluation:
    """Evaluation of a single alternative.

    Attributes:
        name: Alternative name (e.g., "PostgreSQL").
        criteria: List of criteria evidence for this alternative.
        result_count: Number of results associated with this alternative.
    """

    name: str
    criteria: list[CriteriaEvidence] = field(default_factory=list)
    result_count: int = 0


class TradeStudyStrategy(SearchStrategy):
    """Strategy for trade study decision support searches.

    Groups results by alternative and synthesizes criteria evidence
    from any relevant content, enabling trade studies even without
    pre-tagged comparison documents.

    Features:
    - Alternative-centric result grouping
    - Quantitative data extraction and boosting
    - Criteria evidence synthesis from unstructured content

    Example:
        >>> strategy = TradeStudyStrategy()
        >>> output = strategy.format_output(results, {
        ...     "alternatives": ["Option A", "Option B"],
        ...     "criteria": ["cost", "reliability"]
        ... })
        >>> print(output["result_type"])  # "trade_study"
    """

    # Common trade study criteria keywords
    CRITERIA_KEYWORDS = [
        "cost",
        "performance",
        "reliability",
        "maintainability",
        "scalability",
        "weight",
        "schedule",
        "risk",
        "complexity",
        "maturity",
    ]

    # Patterns for quantitative data
    QUANTITATIVE_PATTERNS = [
        r"(\$[\d,]+(?:\.\d{2})?)",  # Cost: $1,234.56
        r"(\d+(?:\.\d+)?\s*(?:ms|sec|min|hr|s))",  # Time: 100ms
        r"(\d+(?:\.\d+)?\s*(?:kg|lbs|g|lb))",  # Weight: 50kg
        r"(\d+(?:\.\d+)?\s*%)",  # Percentage: 99.9%
        r"(\d+(?:\.\d+)?\s*(?:GB|MB|KB|TB))",  # Size: 100GB
    ]

    async def preprocess_query(
        self,
        query: str,
        params: dict[str, Any],
    ) -> SearchQuery:
        """Expand query to cover alternatives and criteria.

        Args:
            query: Base comparison topic.
            params: Should include "alternatives" and optionally "criteria".

        Returns:
            SearchQuery with terms covering each alternative.
        """
        alternatives = params.get("alternatives", [])
        criteria = params.get("criteria", [])

        # Build expanded terms: query + alternative combinations
        expanded_terms = []
        for alt in alternatives:
            expanded_terms.append(f"{query} {alt}")
            for criterion in criteria[:3]:  # Limit criteria expansion
                expanded_terms.append(f"{alt} {criterion}")

        # If no alternatives, just use base query
        if not expanded_terms:
            expanded_terms = [query]

        return SearchQuery(
            original=query,
            expanded_terms=expanded_terms,
            filters=params.get("filters", {}),
        )

    def adjust_ranking(
        self,
        results: list[SearchResult],
    ) -> list[SearchResult]:
        """Boost results with quantitative criteria data.

        Args:
            results: Raw semantic search results.

        Returns:
            Re-ranked results with boosted quantitative content.
        """
        for result in results:
            boost = 1.0
            content_lower = result.content.lower()

            # Boost for quantitative data
            for pattern in self.QUANTITATIVE_PATTERNS:
                if re.search(pattern, result.content):
                    boost += 0.15  # 15% boost for quantitative data
                    break  # Only one boost per quantitative pattern

            # Boost for criteria keywords
            for keyword in self.CRITERIA_KEYWORDS:
                if keyword in content_lower:
                    boost += 0.03  # 3% boost per criterion

            # Boost for comparative language
            if any(word in content_lower for word in ["versus", "compared to", "vs", "better than", "worse than"]):
                boost += 0.1

            result.score = min(1.0, result.score * boost)

        return sorted(results, key=lambda r: r.score, reverse=True)

    def format_output(
        self,
        results: list[SearchResult],
        params: dict[str, Any],
    ) -> dict[str, Any]:
        """Group results by alternative with criteria evidence.

        Args:
            results: Ranked search results.
            params: Must include "alternatives" list.

        Returns:
            Dict with alternatives, each containing criteria evidence.
        """
        alternatives = params.get("alternatives", [])
        criteria = params.get("criteria", self.CRITERIA_KEYWORDS[:5])

        # Group results by alternative
        grouped: dict[str, list[SearchResult]] = defaultdict(list)
        general_results: list[SearchResult] = []

        for result in results:
            assigned = False
            for alt in alternatives:
                if self._content_matches_alternative(result.content, alt):
                    grouped[alt].append(result)
                    assigned = True
                    break

            if not assigned:
                general_results.append(result)

        # Build alternative evaluations
        evaluations = []
        for alt in alternatives:
            alt_results = grouped.get(alt, [])
            criteria_evidence = self._extract_criteria(alt_results, criteria)

            evaluations.append({
                "alternative": alt,
                "criteria": [
                    {
                        "name": ce.name,
                        "evidence": ce.evidence,
                        "value": ce.value,
                        "score": ce.score,
                        "source": ce.source,
                    }
                    for ce in criteria_evidence
                ],
                "result_count": len(alt_results),
                "top_sources": [
                    {
                        "document_title": r.document_title,
                        "section_title": r.section_title,
                        "score": r.score,
                    }
                    for r in alt_results[:3]
                ],
            })

        return {
            "alternatives": evaluations,
            "result_type": "trade_study",
            "total_alternatives": len(evaluations),
            "total_results": len(results),
            "general_results": [
                {
                    "content": r.content[:200] + "..." if len(r.content) > 200 else r.content,
                    "score": r.score,
                    "source": r.document_title,
                }
                for r in general_results[:5]
            ],
        }

    def _content_matches_alternative(
        self,
        content: str,
        alternative: str,
    ) -> bool:
        """Check if content is relevant to an alternative.

        Uses case-insensitive matching with word boundaries.

        Args:
            content: Text content to check.
            alternative: Alternative name to match.

        Returns:
            True if content mentions the alternative.
        """
        # Escape special regex characters in alternative name
        escaped = re.escape(alternative)
        pattern = rf"\b{escaped}\b"
        return bool(re.search(pattern, content, re.IGNORECASE))

    def _extract_criteria(
        self,
        results: list[SearchResult],
        criteria: list[str],
    ) -> list[CriteriaEvidence]:
        """Extract criteria evidence from results.

        Args:
            results: Search results for an alternative.
            criteria: List of criteria to look for.

        Returns:
            List of CriteriaEvidence with extracted information.
        """
        evidence_list: list[CriteriaEvidence] = []

        for criterion in criteria:
            # Find best result mentioning this criterion
            for result in results:
                if criterion.lower() in result.content.lower():
                    # Extract evidence snippet around criterion
                    evidence = self._extract_snippet(result.content, criterion)
                    value = self._extract_value(result.content, criterion)

                    evidence_list.append(
                        CriteriaEvidence(
                            name=criterion,
                            evidence=evidence,
                            value=value,
                            score=result.score,
                            source=result.document_title,
                        )
                    )
                    break  # One evidence per criterion

        return evidence_list

    def _extract_snippet(
        self,
        content: str,
        criterion: str,
        context_chars: int = 150,
    ) -> str:
        """Extract text snippet around criterion mention.

        Args:
            content: Full text content.
            criterion: Criterion keyword to find.
            context_chars: Characters of context to include.

        Returns:
            Text snippet centered on criterion mention.
        """
        pattern = re.escape(criterion)
        match = re.search(pattern, content, re.IGNORECASE)

        if not match:
            return content[:context_chars] + "..."

        start = max(0, match.start() - context_chars // 2)
        end = min(len(content), match.end() + context_chars // 2)

        snippet = content[start:end].strip()
        if start > 0:
            snippet = "..." + snippet
        if end < len(content):
            snippet = snippet + "..."

        return snippet

    def _extract_value(
        self,
        content: str,
        criterion: str,
    ) -> str | None:
        """Extract quantitative value near criterion mention.

        Args:
            content: Text content.
            criterion: Criterion to find values for.

        Returns:
            Extracted value string or None.
        """
        # Find criterion position
        pattern = re.escape(criterion)
        match = re.search(pattern, content, re.IGNORECASE)

        if not match:
            return None

        # Look for quantitative values within 100 chars of criterion
        search_start = max(0, match.start() - 100)
        search_end = min(len(content), match.end() + 100)
        search_text = content[search_start:search_end]

        for value_pattern in self.QUANTITATIVE_PATTERNS:
            if value_match := re.search(value_pattern, search_text):
                return value_match.group(1)

        return None
```

Update `strategies/__init__.py` to export TradeStudyStrategy:
```python
from knowledge_mcp.search.strategies.base import SearchQuery, SearchStrategy
from knowledge_mcp.search.strategies.rcca import RCCAMetadata, RCCAStrategy
from knowledge_mcp.search.strategies.trade import (
    AlternativeEvaluation,
    CriteriaEvidence,
    TradeStudyStrategy,
)

__all__ = [
    "SearchStrategy",
    "SearchQuery",
    "RCCAStrategy",
    "RCCAMetadata",
    "TradeStudyStrategy",
    "CriteriaEvidence",
    "AlternativeEvaluation",
]
```
  </action>
  <verify>
```bash
cd /Users/dunnock/projects/claude-plugins/mcps/knowledge-mcp
poetry run python -c "
from knowledge_mcp.search.strategies import TradeStudyStrategy
strategy = TradeStudyStrategy()
print('TradeStudyStrategy imports OK')
"
poetry run ruff check src/knowledge_mcp/search/strategies/trade.py --select E,F,W
```
  </verify>
  <done>TradeStudyStrategy imports and passes linting</done>
</task>

<task type="auto">
  <name>Task 2: Add unit tests for TradeStudyStrategy</name>
  <files>tests/unit/test_search/test_trade_strategy.py</files>
  <action>
Create comprehensive tests for trade study strategy:

```python
"""Unit tests for Trade Study search strategy."""

from __future__ import annotations

import pytest

from knowledge_mcp.search.models import SearchResult
from knowledge_mcp.search.strategies.trade import (
    CriteriaEvidence,
    TradeStudyStrategy,
)


class TestCriteriaEvidence:
    """Tests for CriteriaEvidence dataclass."""

    def test_defaults(self) -> None:
        """CriteriaEvidence has sensible defaults."""
        ev = CriteriaEvidence(name="cost", evidence="test")
        assert ev.value is None
        assert ev.score == 0.0
        assert ev.source == ""

    def test_with_all_fields(self) -> None:
        """CriteriaEvidence accepts all fields."""
        ev = CriteriaEvidence(
            name="cost",
            evidence="costs $500",
            value="$500",
            score=0.9,
            source="Budget Doc",
        )
        assert ev.value == "$500"


class TestTradeStudyStrategy:
    """Tests for TradeStudyStrategy."""

    @pytest.fixture
    def strategy(self) -> TradeStudyStrategy:
        """Create trade study strategy instance."""
        return TradeStudyStrategy()

    @pytest.mark.asyncio
    async def test_preprocess_expands_alternatives(
        self,
        strategy: TradeStudyStrategy,
    ) -> None:
        """preprocess_query creates terms for each alternative."""
        params = {"alternatives": ["PostgreSQL", "MongoDB"]}
        result = await strategy.preprocess_query("database", params)

        assert "database PostgreSQL" in result.expanded_terms
        assert "database MongoDB" in result.expanded_terms

    @pytest.mark.asyncio
    async def test_preprocess_includes_criteria(
        self,
        strategy: TradeStudyStrategy,
    ) -> None:
        """preprocess_query includes criteria in expansion."""
        params = {
            "alternatives": ["PostgreSQL"],
            "criteria": ["performance", "cost"],
        }
        result = await strategy.preprocess_query("database", params)

        assert any("performance" in term for term in result.expanded_terms)

    @pytest.mark.asyncio
    async def test_preprocess_handles_no_alternatives(
        self,
        strategy: TradeStudyStrategy,
    ) -> None:
        """preprocess_query works without alternatives."""
        result = await strategy.preprocess_query("database comparison", {})

        assert result.original == "database comparison"
        assert len(result.expanded_terms) >= 1

    def test_adjust_ranking_boosts_quantitative(
        self,
        strategy: TradeStudyStrategy,
    ) -> None:
        """adjust_ranking boosts results with quantitative data."""
        results = [
            SearchResult(
                id="1",
                content="PostgreSQL performance is excellent.",
                score=0.8,
            ),
            SearchResult(
                id="2",
                content="PostgreSQL query latency averages 5ms.",
                score=0.75,
            ),
        ]

        ranked = strategy.adjust_ranking(results)

        # Result with quantitative data should be boosted
        quantitative_result = next(r for r in ranked if "5ms" in r.content)
        assert quantitative_result.score > 0.75

    def test_adjust_ranking_boosts_comparative(
        self,
        strategy: TradeStudyStrategy,
    ) -> None:
        """adjust_ranking boosts comparative language."""
        results = [
            SearchResult(
                id="1",
                content="PostgreSQL versus MongoDB comparison.",
                score=0.7,
            ),
        ]

        ranked = strategy.adjust_ranking(results)

        assert ranked[0].score > 0.7

    def test_format_output_groups_by_alternative(
        self,
        strategy: TradeStudyStrategy,
    ) -> None:
        """format_output groups results by alternative."""
        results = [
            SearchResult(
                id="1",
                content="PostgreSQL has strong ACID compliance.",
                score=0.9,
                document_title="DB Guide",
            ),
            SearchResult(
                id="2",
                content="MongoDB excels at document storage.",
                score=0.85,
                document_title="NoSQL Guide",
            ),
        ]
        params = {"alternatives": ["PostgreSQL", "MongoDB"]}

        output = strategy.format_output(results, params)

        assert output["result_type"] == "trade_study"
        assert output["total_alternatives"] == 2

        # Check PostgreSQL alternative
        pg_alt = next(a for a in output["alternatives"] if a["alternative"] == "PostgreSQL")
        assert pg_alt["result_count"] == 1

    def test_format_output_extracts_criteria(
        self,
        strategy: TradeStudyStrategy,
    ) -> None:
        """format_output extracts criteria evidence."""
        results = [
            SearchResult(
                id="1",
                content="PostgreSQL reliability is 99.99% uptime.",
                score=0.9,
                document_title="Reliability Report",
            ),
        ]
        params = {
            "alternatives": ["PostgreSQL"],
            "criteria": ["reliability"],
        }

        output = strategy.format_output(results, params)

        pg_alt = output["alternatives"][0]
        assert len(pg_alt["criteria"]) >= 1
        assert any(c["name"] == "reliability" for c in pg_alt["criteria"])

    def test_format_output_includes_general_results(
        self,
        strategy: TradeStudyStrategy,
    ) -> None:
        """format_output includes unassigned results."""
        results = [
            SearchResult(
                id="1",
                content="Database performance comparison guide.",
                score=0.8,
                document_title="General Guide",
            ),
        ]
        params = {"alternatives": ["PostgreSQL", "MongoDB"]}

        output = strategy.format_output(results, params)

        assert len(output["general_results"]) >= 1

    def test_content_matches_alternative_case_insensitive(
        self,
        strategy: TradeStudyStrategy,
    ) -> None:
        """Alternative matching is case-insensitive."""
        assert strategy._content_matches_alternative("postgresql is great", "PostgreSQL")
        assert strategy._content_matches_alternative("POSTGRESQL is great", "postgresql")

    def test_content_matches_alternative_word_boundary(
        self,
        strategy: TradeStudyStrategy,
    ) -> None:
        """Alternative matching respects word boundaries."""
        assert strategy._content_matches_alternative("PostgreSQL is great", "PostgreSQL")
        assert not strategy._content_matches_alternative("NotPostgreSQL", "PostgreSQL")

    def test_extract_value_finds_cost(
        self,
        strategy: TradeStudyStrategy,
    ) -> None:
        """Extracts cost values near criterion."""
        content = "The cost of PostgreSQL hosting is $500 per month."
        value = strategy._extract_value(content, "cost")
        assert value == "$500"

    def test_extract_value_finds_time(
        self,
        strategy: TradeStudyStrategy,
    ) -> None:
        """Extracts time values near criterion."""
        content = "Query performance averages around 5ms latency."
        value = strategy._extract_value(content, "performance")
        assert value == "5ms"

    def test_extract_snippet_centers_on_criterion(
        self,
        strategy: TradeStudyStrategy,
    ) -> None:
        """Extracts snippet centered on criterion mention."""
        content = "A" * 100 + " cost " + "B" * 100
        snippet = strategy._extract_snippet(content, "cost", context_chars=50)

        assert "cost" in snippet
        assert len(snippet) < len(content)

    def test_format_includes_top_sources(
        self,
        strategy: TradeStudyStrategy,
    ) -> None:
        """Each alternative includes top source documents."""
        results = [
            SearchResult(
                id="1",
                content="PostgreSQL info",
                score=0.9,
                document_title="Doc 1",
                section_title="Section A",
            ),
            SearchResult(
                id="2",
                content="More PostgreSQL info",
                score=0.8,
                document_title="Doc 2",
                section_title="Section B",
            ),
        ]
        params = {"alternatives": ["PostgreSQL"]}

        output = strategy.format_output(results, params)

        pg_alt = output["alternatives"][0]
        assert "top_sources" in pg_alt
        assert len(pg_alt["top_sources"]) == 2
```
  </action>
  <verify>
```bash
cd /Users/dunnock/projects/claude-plugins/mcps/knowledge-mcp
poetry run pytest tests/unit/test_search/test_trade_strategy.py -v
```
  </verify>
  <done>All trade study strategy tests pass</done>
</task>

</tasks>

<verification>
After completing all tasks:

1. Strategy imports and works:
```bash
poetry run python -c "
from knowledge_mcp.search.strategies import TradeStudyStrategy
s = TradeStudyStrategy()
import asyncio
q = asyncio.run(s.preprocess_query('database', {'alternatives': ['PostgreSQL', 'MongoDB']}))
print(f'Query expanded: {len(q.expanded_terms)} terms')
"
```

2. All tests pass:
```bash
poetry run pytest tests/unit/test_search/test_trade_strategy.py -v
```

3. Code quality:
```bash
poetry run ruff check src/knowledge_mcp/search/strategies/trade.py --select E,F,W
```
</verification>

<success_criteria>
- [ ] TradeStudyStrategy extends SearchStrategy ABC
- [ ] preprocess_query expands query with alternatives and criteria
- [ ] adjust_ranking boosts quantitative data and comparative language
- [ ] format_output groups results by alternative with criteria evidence
- [ ] Each alternative includes top_sources and result_count
- [ ] All unit tests pass
- [ ] Code passes ruff linting
</success_criteria>

<output>
After completion, create `.planning/phases/02-workflow-support/02-04-SUMMARY.md`
</output>
